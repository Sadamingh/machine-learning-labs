{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "\n",
    "# The Bag of words presentation\n",
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "\n",
    "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n",
    "\n",
    "tokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n",
    "counting the occurrences of tokens in each document.\n",
    "normalizing and weighting with diminishing importance tokens that occur in the majority of samples / documents.\n",
    "In this scheme, features and samples are defined as follows:\n",
    "\n",
    "each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "the vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n",
    "\n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_set = (\"The sun in the sky is bright.\",\n",
    "    \"We can see the shining sun, the bright sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = CountVectorizer(stop_words = stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = stop_words)\n",
    "train_smatrix = vectorizer.fit_transform(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sky': 2, 'blue': 0, 'sun': 3, 'bright': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n"
     ]
    }
   ],
   "source": [
    "print(train_smatrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 1, 0],\n",
       "        [0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smatrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "test_smatrix = vectorizer.transform(test_set)\n",
    "print(test_smatrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf–idf term weighting\n",
    "\n",
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n",
    "\n",
    "Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: \n",
    "\n",
    "$\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}.$\n",
    "\n",
    "Using the TfidfTransformer’s default settings, TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False) the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\n",
    "\n",
    "$ \\text{idf}(t) = log{\\frac{1 + n_d}{1+\\text{df}(d,t)}} + 1 $\n",
    "\n",
    "where n_d is the total number of documents, and \\text{df}(d,t) is the number of documents that contain term t. The resulting tf-idf vectors are then normalized by the Euclidean norm:\n",
    "\n",
    "$ v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +\n",
    "v{_2}^2 + \\dots + v{_n}^2}}. $\n",
    "\n",
    "This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [1.40546511 1.40546511 1.40546511 1.40546511]\n",
      "Test TFIDF\n",
      "  (0, 3)\t1.4054651081081644\n",
      "  (0, 2)\t1.4054651081081644\n",
      "  (0, 1)\t1.4054651081081644\n",
      "  (1, 3)\t2.8109302162163288\n",
      "  (1, 1)\t1.4054651081081644\n",
      "Train TFIDF\n",
      "  (0, 2)\t1.4054651081081644\n",
      "  (0, 0)\t1.4054651081081644\n",
      "  (1, 3)\t1.4054651081081644\n",
      "  (1, 1)\t1.4054651081081644\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(smooth_idf=True, norm=None)\n",
    "train_tfidf = tfidf.fit_transform(train_smatrix)\n",
    "test_tfidf =  tfidf.transform(test_smatrix)\n",
    "\n",
    "print(\"IDF:\", tfidf.idf_)\n",
    "print(\"Test TFIDF\")\n",
    "print( test_tfidf)\n",
    "\n",
    "print(\"Train TFIDF\")\n",
    "print( train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [1.40546511 1.40546511 1.40546511 1.40546511]\n",
      "Test TFIDF\n",
      "  (0, 3)\t0.5773502691896257\n",
      "  (0, 2)\t0.5773502691896257\n",
      "  (0, 1)\t0.5773502691896257\n",
      "  (1, 3)\t0.894427190999916\n",
      "  (1, 1)\t0.447213595499958\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfTransformer(smooth_idf=True)\n",
    "train_tfidf = tfidf.fit_transform(train_smatrix)\n",
    "test_tfidf =  tfidf.transform(test_smatrix)\n",
    "\n",
    "print(\"IDF:\", tfidf.idf_)\n",
    "print(\"Test TFIDF\")\n",
    "print( test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the NB classifier\n",
    "\n",
    "Now that we have our features, we can train a classifier to try to predict the category of a post. Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task. scikit-learn includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ['sky', 'sun']\n",
    "clf = MultinomialNB().fit(train_tfidf, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before. The difference is that we call transform instead of fit_transform on the transformers, since they have already been fit to the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sun' 'sun']\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the vectorizer => transformer => classifier easier to work with, scikit-learn provides a Pipeline class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(train_set, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sun' 'sun']\n"
     ]
    }
   ],
   "source": [
    "print(text_clf.predict(test_set))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
