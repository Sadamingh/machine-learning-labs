{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras入门1\n",
    "\n",
    "Keras是一个高层神经网络API，可以运行在Tensorflow、Theano以及CNTK等框架上。Keras的优点是简洁，建模速度快，上手容易。如果已经对神经网络有了基础的了解，基于Keras可以快速帮你构架起模型来。也是因为Keras的高度抽象性，对于一些特别复杂的模型个别功能受限，但是对于大多数情况下，Keras已经足够好用了。\n",
    "\n",
    "本文基于一个多变量线性模型的数据，分别由Tensorflow和Keras建立模型进行优化求解，从中你可以对于Tensorflow和Keras的区别，并且感受到Keras带来的便利。\n",
    "\n",
    "## 多变量线性模型\n",
    "\n",
    "现在有一个任务——让机器帮我们找到收入与IQ，工作经验和年龄的关系。为了便于演示和理解，数据是事先“制造”出来的。收入肯定和人的聪明程度有关系，但也不是智商最高的人收入最高，还有一个很重要的因素是工作年限和年龄，假设有这样一个关系：\n",
    "\n",
    "$$income = 0.3*iq + 1.5*experiece + 0.83*age + 5 + noise$$\n",
    "\n",
    "先来随机生成一批数据，然后对数据进行\"清洗\"，要求工作经验不能小于0，年龄不能小于22岁。过滤后数据就整理好了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "np.random.seed(555)\n",
    "samples = 1000\n",
    "iq = np.random.normal(100, 20, samples).astype(int) # IQ\n",
    "user_exp = np.random.normal(20, 10, samples)\n",
    "age = np.random.normal(40, 15, samples).astype(int) # age\n",
    "\n",
    "b = 5\n",
    "noise = np.random.normal(0, 2, samples)\n",
    "\n",
    "Y = 0.3 * iq + 1.5 * user_exp + 0.83 * age + b + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['iq', 'years_experience', 'age']\n",
    "df = pd.DataFrame(list(zip(iq, user_exp, age)), columns=cols)\n",
    "df['income'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iq</th>\n",
       "      <th>years_experience</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>873.000000</td>\n",
       "      <td>873.000000</td>\n",
       "      <td>873.000000</td>\n",
       "      <td>873.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>98.920962</td>\n",
       "      <td>20.679737</td>\n",
       "      <td>42.686140</td>\n",
       "      <td>101.101279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.283697</td>\n",
       "      <td>9.542625</td>\n",
       "      <td>12.357908</td>\n",
       "      <td>18.482897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.121086</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>52.322404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>13.581315</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>87.352662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.658120</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>100.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>112.000000</td>\n",
       "      <td>27.110924</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>113.778953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156.000000</td>\n",
       "      <td>52.890748</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>155.697898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               iq  years_experience         age      income\n",
       "count  873.000000        873.000000  873.000000  873.000000\n",
       "mean    98.920962         20.679737   42.686140  101.101279\n",
       "std     20.283697          9.542625   12.357908   18.482897\n",
       "min     29.000000          0.121086   22.000000   52.322404\n",
       "25%     86.000000         13.581315   33.000000   87.352662\n",
       "50%    100.000000         20.658120   42.000000  100.927100\n",
       "75%    112.000000         27.110924   51.000000  113.778953\n",
       "max    156.000000         52.890748   84.000000  155.697898"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.years_experience >= 0]\n",
    "df = df[df.age >= 22]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整理好数据后分出训练和测试集，使用sklearn的train_test_split方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_data = df[['iq', 'years_experience', 'age']]\n",
    "y_val =  df[['income']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_val,test_size=0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow实现线性模型\n",
    "\n",
    "我们先手工使用TensorFlor实现一个Linear Regressor，还实现minibatch函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#create vars\n",
    "W = tf.get_variable('W', [3,1], initializer = tf.zeros_initializer())\n",
    "b = tf.get_variable('b', [1,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "# create input placeholders\n",
    "X = tf.placeholder('float32', name='X')\n",
    "Y = tf.placeholder('float32', name='Y_true')\n",
    "\n",
    "# create linear model\n",
    "#yhat = tf.reshape(tf.matmul(X,W) + b, [-1,], name='yhat')\n",
    "yhat = tf.matmul(X,W) + b\n",
    "\n",
    "mse = tf.reduce_mean(tf.squared_difference(Y, yhat), name='mse')\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 8349.256727\n",
      "Cost in test set after epoch 0: 9753.441961\n",
      "Cost after epoch 100: 73.009706\n",
      "Cost in test set after epoch 100: 68.277555\n",
      "Cost after epoch 200: 16.396915\n",
      "Cost in test set after epoch 200: 15.680660\n",
      "Cost after epoch 300: 3.585278\n",
      "Cost in test set after epoch 300: 4.647265\n",
      "Cost after epoch 400: 3.445812\n",
      "Cost in test set after epoch 400: 4.557039\n",
      "Weights : [array([[0.31692702],\n",
      "       [1.5301745 ],\n",
      "       [0.85716254]], dtype=float32), array([[1.4373599]], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on the test set is 4.4552490331308245\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics  \n",
    "def predict_cost(W, b, x_test, y_test):\n",
    "    y_pred = x_test @ W + b\n",
    "    return metrics.mean_squared_error(y_pred, y_test)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    # init vars\n",
    "    sess.run(init)\n",
    "    num_epochs = 500\n",
    "    minibatch_size = 32\n",
    "    print_cost = True\n",
    "    m = x_data.shape[0]\n",
    "    costs = []\n",
    "    pred_costs = []\n",
    "    seed = 100\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        #print(num_minibatches)\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train.values, y_train.values, minibatch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            _, minibatch_cost = sess.run([optimizer, mse], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "            \n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "        # Print the cost every epoch\n",
    "        if print_cost == True and epoch % 100 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            \n",
    "            Wt, bt = sess.run([W,b])\n",
    "            pred_cost = predict_cost(Wt, bt, X_test.values, y_test.values )                  \n",
    "            print (\"Cost in test set after epoch %i: %f\" % (epoch, pred_cost))   \n",
    "                \n",
    "                \n",
    "        if print_cost == True and epoch % 5 == 0:\n",
    "            costs.append(epoch_cost)\n",
    "            \n",
    "            Wt, bt = sess.run([W,b])\n",
    "            pred_cost = predict_cost(Wt, bt, X_test.values, y_test.values )\n",
    "            pred_costs.append(pred_cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    print(\"Weights : {}\".format(sess.run([W,b])))\n",
    "    plt.plot(np.squeeze(costs), label=\"train\")\n",
    "    plt.plot(np.squeeze(pred_costs), label=\"test\")\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    Wt, bt = sess.run([W,b])\n",
    "    pred_cost = predict_cost(Wt, bt, X_test.values, y_test.values )\n",
    "    print(\"Cost on the test set is {}\".format(pred_cost))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras模型\n",
    "\n",
    "Keras建模主要有几下几步：\n",
    "1. 使用Keras内置的API实现前向传播计算方法\n",
    "2. Compile the model by calling model.compile(optimizer = \"...\", loss = \"...\", metrics = [\"accuracy\"])\n",
    "3. Train the model on train data by calling model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)\n",
    "4. Test the model on test data by calling model.evaluate(x = ..., y = ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "# visualize model structure\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearModel(input_shape):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(1, activation='linear', name='fc')(X_input)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name='linearModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linearModel(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"134pt\" viewBox=\"0.00 0.00 211.51 134.00\" width=\"212pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 130)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-130 207.5107,-130 207.5107,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4653853328 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4653853328</title>\n",
       "<polygon fill=\"none\" points=\"0,-81.5 0,-125.5 203.5107,-125.5 203.5107,-81.5 0,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39.0967\" y=\"-99.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"78.1934,-81.5 78.1934,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.0278\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"78.1934,-103.5 133.8623,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.0278\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"133.8623,-81.5 133.8623,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.6865\" y=\"-110.3\">(None, 3)</text>\n",
       "<polyline fill=\"none\" points=\"133.8623,-103.5 203.5107,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.6865\" y=\"-88.3\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 4653853776 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4653853776</title>\n",
       "<polygon fill=\"none\" points=\"13.6035,-.5 13.6035,-44.5 189.9072,-44.5 189.9072,-.5 13.6035,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39.0967\" y=\"-18.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"64.5898,-.5 64.5898,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.4243\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64.5898,-22.5 120.2588,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.4243\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120.2588,-.5 120.2588,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.083\" y=\"-29.3\">(None, 3)</text>\n",
       "<polyline fill=\"none\" points=\"120.2588,-22.5 189.9072,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.083\" y=\"-7.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 4653853328&#45;&gt;4653853776 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4653853328-&gt;4653853776</title>\n",
       "<path d=\"M101.7554,-81.3664C101.7554,-73.1516 101.7554,-63.6579 101.7554,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"105.2555,-54.6068 101.7554,-44.6068 98.2555,-54.6069 105.2555,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt =  Adam(lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer = \"adam\", loss = \"mean_absolute_error\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 698 samples, validate on 175 samples\n",
      "Epoch 1/500\n",
      "698/698 [==============================] - 0s 207us/step - loss: 15.2865 - acc: 0.0000e+00 - val_loss: 13.3328 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 13.4568 - acc: 0.0000e+00 - val_loss: 11.8061 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 12.2828 - acc: 0.0000e+00 - val_loss: 10.8707 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 11.6250 - acc: 0.0000e+00 - val_loss: 10.3064 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 11.2661 - acc: 0.0000e+00 - val_loss: 9.9996 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 11.0505 - acc: 0.0000e+00 - val_loss: 9.7950 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 10.8991 - acc: 0.0000e+00 - val_loss: 9.6110 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 10.7538 - acc: 0.0000e+00 - val_loss: 9.4837 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 10.6066 - acc: 0.0000e+00 - val_loss: 9.3379 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 10.4557 - acc: 0.0000e+00 - val_loss: 9.2014 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 10.2988 - acc: 0.0000e+00 - val_loss: 9.0547 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 10.1385 - acc: 0.0000e+00 - val_loss: 8.9106 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 9.9723 - acc: 0.0000e+00 - val_loss: 8.7578 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 9.8034 - acc: 0.0000e+00 - val_loss: 8.6059 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 9.6376 - acc: 0.0000e+00 - val_loss: 8.4414 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 9.4568 - acc: 0.0000e+00 - val_loss: 8.2873 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 9.2780 - acc: 0.0000e+00 - val_loss: 8.1211 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 9.0926 - acc: 0.0000e+00 - val_loss: 7.9627 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 8.9082 - acc: 0.0000e+00 - val_loss: 7.7924 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 8.7199 - acc: 0.0000e+00 - val_loss: 7.6206 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 8.5310 - acc: 0.0000e+00 - val_loss: 7.4541 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 8.3451 - acc: 0.0000e+00 - val_loss: 7.2899 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 8.1549 - acc: 0.0000e+00 - val_loss: 7.1129 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 7.9577 - acc: 0.0000e+00 - val_loss: 6.9393 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 7.7718 - acc: 0.0000e+00 - val_loss: 6.7688 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 7.5690 - acc: 0.0000e+00 - val_loss: 6.5989 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 7.3713 - acc: 0.0000e+00 - val_loss: 6.4239 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 7.1755 - acc: 0.0000e+00 - val_loss: 6.2510 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 6.9795 - acc: 0.0000e+00 - val_loss: 6.0711 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 6.7756 - acc: 0.0000e+00 - val_loss: 5.8929 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 6.5737 - acc: 0.0000e+00 - val_loss: 5.7062 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 6.3734 - acc: 0.0000e+00 - val_loss: 5.5290 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 6.1777 - acc: 0.0000e+00 - val_loss: 5.3467 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 5.9768 - acc: 0.0000e+00 - val_loss: 5.1741 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 5.7819 - acc: 0.0000e+00 - val_loss: 4.9860 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 5.5814 - acc: 0.0000e+00 - val_loss: 4.8162 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 5.3771 - acc: 0.0000e+00 - val_loss: 4.6446 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 5.1878 - acc: 0.0000e+00 - val_loss: 4.4542 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 4.9950 - acc: 0.0000e+00 - val_loss: 4.3150 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 4.8170 - acc: 0.0000e+00 - val_loss: 4.1180 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 4.6135 - acc: 0.0000e+00 - val_loss: 3.9721 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 4.4282 - acc: 0.0000e+00 - val_loss: 3.8087 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 4.2268 - acc: 0.0000e+00 - val_loss: 3.6362 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 4.0416 - acc: 0.0000e+00 - val_loss: 3.4905 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 3.8691 - acc: 0.0000e+00 - val_loss: 3.3198 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 3.6807 - acc: 0.0000e+00 - val_loss: 3.1894 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 3.5086 - acc: 0.0000e+00 - val_loss: 3.0591 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 3.3370 - acc: 0.0000e+00 - val_loss: 2.9036 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 3.1735 - acc: 0.0000e+00 - val_loss: 2.7833 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 3.0140 - acc: 0.0000e+00 - val_loss: 2.6646 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 2.8588 - acc: 0.0000e+00 - val_loss: 2.5496 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 2.7135 - acc: 0.0000e+00 - val_loss: 2.4433 - val_acc: 0.0000e+00\n",
      "Epoch 53/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 2.5857 - acc: 0.0000e+00 - val_loss: 2.3452 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 2.4513 - acc: 0.0000e+00 - val_loss: 2.2546 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 2.3313 - acc: 0.0000e+00 - val_loss: 2.1719 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 2.2281 - acc: 0.0000e+00 - val_loss: 2.0840 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 2.1308 - acc: 0.0000e+00 - val_loss: 2.0244 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 2.0567 - acc: 0.0000e+00 - val_loss: 1.9568 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.9814 - acc: 0.0000e+00 - val_loss: 1.9018 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.9230 - acc: 0.0000e+00 - val_loss: 1.8701 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.8887 - acc: 0.0000e+00 - val_loss: 1.8746 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.8197 - acc: 0.0000e+00 - val_loss: 1.8119 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.7850 - acc: 0.0000e+00 - val_loss: 1.8116 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.7535 - acc: 0.0000e+00 - val_loss: 1.7961 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.7220 - acc: 0.0000e+00 - val_loss: 1.7745 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.7088 - acc: 0.0000e+00 - val_loss: 1.7716 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6913 - acc: 0.0000e+00 - val_loss: 1.7810 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6772 - acc: 0.0000e+00 - val_loss: 1.7836 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6689 - acc: 0.0000e+00 - val_loss: 1.7734 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6686 - acc: 0.0000e+00 - val_loss: 1.7913 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6604 - acc: 0.0000e+00 - val_loss: 1.7838 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6568 - acc: 0.0000e+00 - val_loss: 1.7878 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6579 - acc: 0.0000e+00 - val_loss: 1.7979 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6553 - acc: 0.0000e+00 - val_loss: 1.8108 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6534 - acc: 0.0000e+00 - val_loss: 1.7856 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6508 - acc: 0.0000e+00 - val_loss: 1.7995 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6507 - acc: 0.0000e+00 - val_loss: 1.8020 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6508 - acc: 0.0000e+00 - val_loss: 1.8040 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6475 - acc: 0.0000e+00 - val_loss: 1.7921 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "698/698 [==============================] - 0s 50us/step - loss: 1.6529 - acc: 0.0000e+00 - val_loss: 1.8108 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.6463 - acc: 0.0000e+00 - val_loss: 1.8043 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6484 - acc: 0.0000e+00 - val_loss: 1.8107 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "698/698 [==============================] - 0s 59us/step - loss: 1.6462 - acc: 0.0000e+00 - val_loss: 1.7934 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "698/698 [==============================] - 0s 44us/step - loss: 1.6534 - acc: 0.0000e+00 - val_loss: 1.7966 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6506 - acc: 0.0000e+00 - val_loss: 1.8236 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6453 - acc: 0.0000e+00 - val_loss: 1.7927 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6571 - acc: 0.0000e+00 - val_loss: 1.8352 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6466 - acc: 0.0000e+00 - val_loss: 1.7889 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6565 - acc: 0.0000e+00 - val_loss: 1.8418 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6495 - acc: 0.0000e+00 - val_loss: 1.8010 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6477 - acc: 0.0000e+00 - val_loss: 1.8361 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6436 - acc: 0.0000e+00 - val_loss: 1.7877 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6475 - acc: 0.0000e+00 - val_loss: 1.8017 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6421 - acc: 0.0000e+00 - val_loss: 1.7976 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6412 - acc: 0.0000e+00 - val_loss: 1.8198 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6461 - acc: 0.0000e+00 - val_loss: 1.8127 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6452 - acc: 0.0000e+00 - val_loss: 1.7897 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6539 - acc: 0.0000e+00 - val_loss: 1.7933 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6468 - acc: 0.0000e+00 - val_loss: 1.7932 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6469 - acc: 0.0000e+00 - val_loss: 1.7965 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6409 - acc: 0.0000e+00 - val_loss: 1.8101 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6407 - acc: 0.0000e+00 - val_loss: 1.8159 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6429 - acc: 0.0000e+00 - val_loss: 1.8177 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6393 - acc: 0.0000e+00 - val_loss: 1.8068 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6373 - acc: 0.0000e+00 - val_loss: 1.7891 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6407 - acc: 0.0000e+00 - val_loss: 1.8011 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6377 - acc: 0.0000e+00 - val_loss: 1.8208 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6466 - acc: 0.0000e+00 - val_loss: 1.8062 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6446 - acc: 0.0000e+00 - val_loss: 1.7915 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6419 - acc: 0.0000e+00 - val_loss: 1.8053 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "698/698 [==============================] - 0s 55us/step - loss: 1.6368 - acc: 0.0000e+00 - val_loss: 1.7943 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6409 - acc: 0.0000e+00 - val_loss: 1.7953 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6415 - acc: 0.0000e+00 - val_loss: 1.8103 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6471 - acc: 0.0000e+00 - val_loss: 1.8025 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 0s 29us/step - loss: 1.6387 - acc: 0.0000e+00 - val_loss: 1.8031 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.6363 - acc: 0.0000e+00 - val_loss: 1.8078 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6353 - acc: 0.0000e+00 - val_loss: 1.7966 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6365 - acc: 0.0000e+00 - val_loss: 1.8132 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6367 - acc: 0.0000e+00 - val_loss: 1.8029 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6365 - acc: 0.0000e+00 - val_loss: 1.7922 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6443 - acc: 0.0000e+00 - val_loss: 1.8013 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6426 - acc: 0.0000e+00 - val_loss: 1.7967 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6349 - acc: 0.0000e+00 - val_loss: 1.7926 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6344 - acc: 0.0000e+00 - val_loss: 1.7938 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6425 - acc: 0.0000e+00 - val_loss: 1.8392 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6357 - acc: 0.0000e+00 - val_loss: 1.7885 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6323 - acc: 0.0000e+00 - val_loss: 1.7970 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6331 - acc: 0.0000e+00 - val_loss: 1.7843 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6348 - acc: 0.0000e+00 - val_loss: 1.8043 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6346 - acc: 0.0000e+00 - val_loss: 1.7963 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6308 - acc: 0.0000e+00 - val_loss: 1.7888 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6291 - acc: 0.0000e+00 - val_loss: 1.8031 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6338 - acc: 0.0000e+00 - val_loss: 1.7988 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6311 - acc: 0.0000e+00 - val_loss: 1.8139 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6292 - acc: 0.0000e+00 - val_loss: 1.7819 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.6324 - acc: 0.0000e+00 - val_loss: 1.8420 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6407 - acc: 0.0000e+00 - val_loss: 1.7932 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.6303 - acc: 0.0000e+00 - val_loss: 1.8088 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.6311 - acc: 0.0000e+00 - val_loss: 1.8027 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.6271 - acc: 0.0000e+00 - val_loss: 1.7875 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.6317 - acc: 0.0000e+00 - val_loss: 1.8335 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6396 - acc: 0.0000e+00 - val_loss: 1.8110 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6311 - acc: 0.0000e+00 - val_loss: 1.7833 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6230 - acc: 0.0000e+00 - val_loss: 1.8213 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6320 - acc: 0.0000e+00 - val_loss: 1.7819 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6350 - acc: 0.0000e+00 - val_loss: 1.7936 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6241 - acc: 0.0000e+00 - val_loss: 1.7781 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6280 - acc: 0.0000e+00 - val_loss: 1.8129 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6248 - acc: 0.0000e+00 - val_loss: 1.7830 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6273 - acc: 0.0000e+00 - val_loss: 1.7772 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6271 - acc: 0.0000e+00 - val_loss: 1.7898 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6222 - acc: 0.0000e+00 - val_loss: 1.7806 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6262 - acc: 0.0000e+00 - val_loss: 1.7767 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6332 - acc: 0.0000e+00 - val_loss: 1.7835 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6301 - acc: 0.0000e+00 - val_loss: 1.8161 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6471 - acc: 0.0000e+00 - val_loss: 1.7750 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6224 - acc: 0.0000e+00 - val_loss: 1.8255 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6220 - acc: 0.0000e+00 - val_loss: 1.7793 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6238 - acc: 0.0000e+00 - val_loss: 1.7988 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6203 - acc: 0.0000e+00 - val_loss: 1.7765 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6245 - acc: 0.0000e+00 - val_loss: 1.7889 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6229 - acc: 0.0000e+00 - val_loss: 1.7881 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6215 - acc: 0.0000e+00 - val_loss: 1.7787 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6209 - acc: 0.0000e+00 - val_loss: 1.7751 - val_acc: 0.0000e+00\n",
      "Epoch 165/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6221 - acc: 0.0000e+00 - val_loss: 1.7952 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6228 - acc: 0.0000e+00 - val_loss: 1.7914 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6230 - acc: 0.0000e+00 - val_loss: 1.7893 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6181 - acc: 0.0000e+00 - val_loss: 1.7892 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6238 - acc: 0.0000e+00 - val_loss: 1.7756 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6204 - acc: 0.0000e+00 - val_loss: 1.7847 - val_acc: 0.0000e+00\n",
      "Epoch 171/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6251 - acc: 0.0000e+00 - val_loss: 1.7728 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6214 - acc: 0.0000e+00 - val_loss: 1.7831 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6199 - acc: 0.0000e+00 - val_loss: 1.7837 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6217 - acc: 0.0000e+00 - val_loss: 1.8081 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6207 - acc: 0.0000e+00 - val_loss: 1.7951 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6223 - acc: 0.0000e+00 - val_loss: 1.8132 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6171 - acc: 0.0000e+00 - val_loss: 1.7702 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6151 - acc: 0.0000e+00 - val_loss: 1.7952 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6171 - acc: 0.0000e+00 - val_loss: 1.7777 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6134 - acc: 0.0000e+00 - val_loss: 1.7954 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6168 - acc: 0.0000e+00 - val_loss: 1.7683 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.6258 - acc: 0.0000e+0 - 0s 31us/step - loss: 1.6278 - acc: 0.0000e+00 - val_loss: 1.7729 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.6271 - acc: 0.0000e+00 - val_loss: 1.7773 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6209 - acc: 0.0000e+00 - val_loss: 1.7682 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6206 - acc: 0.0000e+00 - val_loss: 1.7976 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6202 - acc: 0.0000e+00 - val_loss: 1.7998 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6182 - acc: 0.0000e+00 - val_loss: 1.7762 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6152 - acc: 0.0000e+00 - val_loss: 1.7670 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6221 - acc: 0.0000e+00 - val_loss: 1.7833 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6188 - acc: 0.0000e+00 - val_loss: 1.7665 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6177 - acc: 0.0000e+00 - val_loss: 1.8009 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6260 - acc: 0.0000e+00 - val_loss: 1.8106 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6132 - acc: 0.0000e+00 - val_loss: 1.7684 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6193 - acc: 0.0000e+00 - val_loss: 1.7806 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6112 - acc: 0.0000e+00 - val_loss: 1.7826 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6122 - acc: 0.0000e+00 - val_loss: 1.7640 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6235 - acc: 0.0000e+00 - val_loss: 1.7794 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6120 - acc: 0.0000e+00 - val_loss: 1.7892 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6096 - acc: 0.0000e+00 - val_loss: 1.7721 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6107 - acc: 0.0000e+00 - val_loss: 1.7811 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6110 - acc: 0.0000e+00 - val_loss: 1.7922 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6191 - acc: 0.0000e+00 - val_loss: 1.7679 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6133 - acc: 0.0000e+00 - val_loss: 1.7726 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6116 - acc: 0.0000e+00 - val_loss: 1.7751 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6170 - acc: 0.0000e+00 - val_loss: 1.7639 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6122 - acc: 0.0000e+00 - val_loss: 1.7823 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6190 - acc: 0.0000e+00 - val_loss: 1.8006 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6177 - acc: 0.0000e+00 - val_loss: 1.7876 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6102 - acc: 0.0000e+00 - val_loss: 1.7744 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6081 - acc: 0.0000e+00 - val_loss: 1.7667 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6143 - acc: 0.0000e+00 - val_loss: 1.7600 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6279 - acc: 0.0000e+00 - val_loss: 1.7694 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6105 - acc: 0.0000e+00 - val_loss: 1.7809 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6076 - acc: 0.0000e+00 - val_loss: 1.7628 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6071 - acc: 0.0000e+00 - val_loss: 1.8040 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6125 - acc: 0.0000e+00 - val_loss: 1.7635 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6058 - acc: 0.0000e+00 - val_loss: 1.7690 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6108 - acc: 0.0000e+00 - val_loss: 1.7660 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6087 - acc: 0.0000e+00 - val_loss: 1.7572 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6097 - acc: 0.0000e+00 - val_loss: 1.7993 - val_acc: 0.0000e+00\n",
      "Epoch 221/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6191 - acc: 0.0000e+00 - val_loss: 1.7816 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6034 - acc: 0.0000e+00 - val_loss: 1.7646 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6076 - acc: 0.0000e+00 - val_loss: 1.7956 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6084 - acc: 0.0000e+00 - val_loss: 1.7748 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6070 - acc: 0.0000e+00 - val_loss: 1.7631 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6084 - acc: 0.0000e+00 - val_loss: 1.7848 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6042 - acc: 0.0000e+00 - val_loss: 1.7596 - val_acc: 0.0000e+00\n",
      "Epoch 228/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 0s 34us/step - loss: 1.6051 - acc: 0.0000e+00 - val_loss: 1.7585 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6065 - acc: 0.0000e+00 - val_loss: 1.8058 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6077 - acc: 0.0000e+00 - val_loss: 1.7566 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6043 - acc: 0.0000e+00 - val_loss: 1.7637 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6091 - acc: 0.0000e+00 - val_loss: 1.7654 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.6021 - acc: 0.0000e+00 - val_loss: 1.7789 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6067 - acc: 0.0000e+00 - val_loss: 1.7637 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6043 - acc: 0.0000e+00 - val_loss: 1.7920 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6093 - acc: 0.0000e+00 - val_loss: 1.7548 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6025 - acc: 0.0000e+00 - val_loss: 1.8034 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6108 - acc: 0.0000e+00 - val_loss: 1.7588 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6001 - acc: 0.0000e+00 - val_loss: 1.7653 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6021 - acc: 0.0000e+00 - val_loss: 1.7639 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6018 - acc: 0.0000e+00 - val_loss: 1.7795 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6091 - acc: 0.0000e+00 - val_loss: 1.7804 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5998 - acc: 0.0000e+00 - val_loss: 1.7559 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5996 - acc: 0.0000e+00 - val_loss: 1.7595 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6063 - acc: 0.0000e+00 - val_loss: 1.7506 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6068 - acc: 0.0000e+00 - val_loss: 1.7860 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6076 - acc: 0.0000e+00 - val_loss: 1.7738 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5993 - acc: 0.0000e+00 - val_loss: 1.7570 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5977 - acc: 0.0000e+00 - val_loss: 1.7844 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5962 - acc: 0.0000e+00 - val_loss: 1.7543 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5967 - acc: 0.0000e+00 - val_loss: 1.7860 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5991 - acc: 0.0000e+00 - val_loss: 1.7525 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6003 - acc: 0.0000e+00 - val_loss: 1.7643 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5973 - acc: 0.0000e+00 - val_loss: 1.7569 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6023 - acc: 0.0000e+00 - val_loss: 1.7783 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5967 - acc: 0.0000e+00 - val_loss: 1.7488 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5966 - acc: 0.0000e+00 - val_loss: 1.8139 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6066 - acc: 0.0000e+00 - val_loss: 1.7494 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.6062 - acc: 0.0000e+00 - val_loss: 1.7455 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6116 - acc: 0.0000e+00 - val_loss: 1.7527 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5978 - acc: 0.0000e+00 - val_loss: 1.7742 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5974 - acc: 0.0000e+00 - val_loss: 1.7462 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6122 - acc: 0.0000e+00 - val_loss: 1.7559 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5983 - acc: 0.0000e+00 - val_loss: 1.7725 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5951 - acc: 0.0000e+00 - val_loss: 1.7447 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6161 - acc: 0.0000e+00 - val_loss: 1.7548 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.6118 - acc: 0.0000e+00 - val_loss: 1.8120 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6135 - acc: 0.0000e+00 - val_loss: 1.7733 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6046 - acc: 0.0000e+00 - val_loss: 1.7572 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5958 - acc: 0.0000e+00 - val_loss: 1.7477 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5928 - acc: 0.0000e+00 - val_loss: 1.7940 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6151 - acc: 0.0000e+00 - val_loss: 1.7641 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5971 - acc: 0.0000e+00 - val_loss: 1.7522 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5931 - acc: 0.0000e+00 - val_loss: 1.7748 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5974 - acc: 0.0000e+00 - val_loss: 1.7742 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5936 - acc: 0.0000e+00 - val_loss: 1.7539 - val_acc: 0.0000e+00\n",
      "Epoch 277/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5918 - acc: 0.0000e+00 - val_loss: 1.7530 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5951 - acc: 0.0000e+00 - val_loss: 1.7413 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5963 - acc: 0.0000e+00 - val_loss: 1.7567 - val_acc: 0.0000e+00\n",
      "Epoch 280/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5947 - acc: 0.0000e+00 - val_loss: 1.7506 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5928 - acc: 0.0000e+00 - val_loss: 1.7983 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5912 - acc: 0.0000e+00 - val_loss: 1.7405 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5909 - acc: 0.0000e+00 - val_loss: 1.7697 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5990 - acc: 0.0000e+00 - val_loss: 1.7613 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5877 - acc: 0.0000e+00 - val_loss: 1.7440 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5884 - acc: 0.0000e+00 - val_loss: 1.7839 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5968 - acc: 0.0000e+00 - val_loss: 1.7404 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5892 - acc: 0.0000e+00 - val_loss: 1.7697 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5901 - acc: 0.0000e+00 - val_loss: 1.7469 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5987 - acc: 0.0000e+00 - val_loss: 1.7424 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5878 - acc: 0.0000e+00 - val_loss: 1.7647 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5893 - acc: 0.0000e+00 - val_loss: 1.7504 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5863 - acc: 0.0000e+00 - val_loss: 1.7538 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5909 - acc: 0.0000e+00 - val_loss: 1.7380 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5947 - acc: 0.0000e+00 - val_loss: 1.7814 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5894 - acc: 0.0000e+00 - val_loss: 1.7449 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5995 - acc: 0.0000e+00 - val_loss: 1.7755 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5863 - acc: 0.0000e+00 - val_loss: 1.7407 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5882 - acc: 0.0000e+00 - val_loss: 1.7914 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5916 - acc: 0.0000e+00 - val_loss: 1.7370 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6097 - acc: 0.0000e+00 - val_loss: 1.7540 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5861 - acc: 0.0000e+00 - val_loss: 1.7554 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5875 - acc: 0.0000e+00 - val_loss: 1.7590 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5924 - acc: 0.0000e+00 - val_loss: 1.7629 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5905 - acc: 0.0000e+00 - val_loss: 1.7349 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5921 - acc: 0.0000e+00 - val_loss: 1.7505 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5860 - acc: 0.0000e+00 - val_loss: 1.7420 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5870 - acc: 0.0000e+00 - val_loss: 1.7599 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5849 - acc: 0.0000e+00 - val_loss: 1.7480 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5874 - acc: 0.0000e+00 - val_loss: 1.7586 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5871 - acc: 0.0000e+00 - val_loss: 1.7478 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5889 - acc: 0.0000e+00 - val_loss: 1.7556 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5882 - acc: 0.0000e+00 - val_loss: 1.7528 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5845 - acc: 0.0000e+00 - val_loss: 1.7461 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5843 - acc: 0.0000e+00 - val_loss: 1.7737 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5797 - acc: 0.0000e+00 - val_loss: 1.7327 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5966 - acc: 0.0000e+00 - val_loss: 1.7656 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5862 - acc: 0.0000e+00 - val_loss: 1.7663 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5843 - acc: 0.0000e+00 - val_loss: 1.7319 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5902 - acc: 0.0000e+00 - val_loss: 1.7368 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5888 - acc: 0.0000e+00 - val_loss: 1.7594 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5948 - acc: 0.0000e+00 - val_loss: 1.7823 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.6003 - acc: 0.0000e+00 - val_loss: 1.7419 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5792 - acc: 0.0000e+00 - val_loss: 1.7730 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5832 - acc: 0.0000e+00 - val_loss: 1.7466 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5811 - acc: 0.0000e+00 - val_loss: 1.7521 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5846 - acc: 0.0000e+00 - val_loss: 1.7610 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5851 - acc: 0.0000e+00 - val_loss: 1.7312 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5872 - acc: 0.0000e+00 - val_loss: 1.7812 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5781 - acc: 0.0000e+00 - val_loss: 1.7305 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5917 - acc: 0.0000e+00 - val_loss: 1.7759 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5783 - acc: 0.0000e+00 - val_loss: 1.7390 - val_acc: 0.0000e+00\n",
      "Epoch 333/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5932 - acc: 0.0000e+00 - val_loss: 1.8076 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.6006 - acc: 0.0000e+00 - val_loss: 1.7337 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5751 - acc: 0.0000e+00 - val_loss: 1.7705 - val_acc: 0.0000e+00\n",
      "Epoch 336/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5797 - acc: 0.0000e+00 - val_loss: 1.7309 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5933 - acc: 0.0000e+00 - val_loss: 1.7341 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5777 - acc: 0.0000e+00 - val_loss: 1.8110 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.6009 - acc: 0.0000e+00 - val_loss: 1.7442 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5808 - acc: 0.0000e+00 - val_loss: 1.7571 - val_acc: 0.0000e+00\n",
      "Epoch 341/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5777 - acc: 0.0000e+00 - val_loss: 1.7456 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5862 - acc: 0.0000e+00 - val_loss: 1.7272 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5844 - acc: 0.0000e+00 - val_loss: 1.7282 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5782 - acc: 0.0000e+00 - val_loss: 1.7458 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5973 - acc: 0.0000e+00 - val_loss: 1.7241 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5759 - acc: 0.0000e+00 - val_loss: 1.7728 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5840 - acc: 0.0000e+00 - val_loss: 1.7312 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5738 - acc: 0.0000e+00 - val_loss: 1.7442 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5772 - acc: 0.0000e+00 - val_loss: 1.7309 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5777 - acc: 0.0000e+00 - val_loss: 1.7691 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5772 - acc: 0.0000e+00 - val_loss: 1.7246 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5781 - acc: 0.0000e+00 - val_loss: 1.7379 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5757 - acc: 0.0000e+00 - val_loss: 1.7625 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5832 - acc: 0.0000e+00 - val_loss: 1.7269 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5769 - acc: 0.0000e+00 - val_loss: 1.7570 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5729 - acc: 0.0000e+00 - val_loss: 1.7326 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5749 - acc: 0.0000e+00 - val_loss: 1.7317 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5782 - acc: 0.0000e+00 - val_loss: 1.7309 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5838 - acc: 0.0000e+00 - val_loss: 1.7332 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5843 - acc: 0.0000e+00 - val_loss: 1.7356 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5732 - acc: 0.0000e+00 - val_loss: 1.7451 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5812 - acc: 0.0000e+00 - val_loss: 1.7451 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5771 - acc: 0.0000e+00 - val_loss: 1.7357 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5744 - acc: 0.0000e+00 - val_loss: 1.8023 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6008 - acc: 0.0000e+00 - val_loss: 1.7209 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5721 - acc: 0.0000e+00 - val_loss: 1.7640 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5805 - acc: 0.0000e+00 - val_loss: 1.7439 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5709 - acc: 0.0000e+00 - val_loss: 1.7335 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5773 - acc: 0.0000e+00 - val_loss: 1.7670 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5744 - acc: 0.0000e+00 - val_loss: 1.7178 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5810 - acc: 0.0000e+00 - val_loss: 1.7499 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5742 - acc: 0.0000e+00 - val_loss: 1.7447 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5822 - acc: 0.0000e+00 - val_loss: 1.7419 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5740 - acc: 0.0000e+00 - val_loss: 1.7268 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5721 - acc: 0.0000e+00 - val_loss: 1.7252 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5729 - acc: 0.0000e+00 - val_loss: 1.7578 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "698/698 [==============================] - 0s 25us/step - loss: 1.5703 - acc: 0.0000e+00 - val_loss: 1.7258 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5788 - acc: 0.0000e+00 - val_loss: 1.7412 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5721 - acc: 0.0000e+00 - val_loss: 1.7312 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5733 - acc: 0.0000e+00 - val_loss: 1.7472 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5733 - acc: 0.0000e+00 - val_loss: 1.7272 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5769 - acc: 0.0000e+00 - val_loss: 1.7214 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5760 - acc: 0.0000e+00 - val_loss: 1.7419 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5728 - acc: 0.0000e+00 - val_loss: 1.7565 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5645 - acc: 0.0000e+00 - val_loss: 1.7164 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "698/698 [==============================] - 0s 41us/step - loss: 1.5782 - acc: 0.0000e+00 - val_loss: 1.7502 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5694 - acc: 0.0000e+00 - val_loss: 1.7169 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5710 - acc: 0.0000e+00 - val_loss: 1.7243 - val_acc: 0.0000e+00\n",
      "Epoch 389/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5697 - acc: 0.0000e+00 - val_loss: 1.7543 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5705 - acc: 0.0000e+00 - val_loss: 1.7415 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5689 - acc: 0.0000e+00 - val_loss: 1.7263 - val_acc: 0.0000e+00\n",
      "Epoch 392/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5765 - acc: 0.0000e+00 - val_loss: 1.7711 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5739 - acc: 0.0000e+00 - val_loss: 1.7382 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5722 - acc: 0.0000e+00 - val_loss: 1.7170 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5718 - acc: 0.0000e+00 - val_loss: 1.7524 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5701 - acc: 0.0000e+00 - val_loss: 1.7393 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5652 - acc: 0.0000e+00 - val_loss: 1.7449 - val_acc: 0.0000e+00\n",
      "Epoch 398/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5665 - acc: 0.0000e+00 - val_loss: 1.7350 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5902 - acc: 0.0000e+00 - val_loss: 1.7869 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5615 - acc: 0.0000e+00 - val_loss: 1.7144 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5684 - acc: 0.0000e+00 - val_loss: 1.7514 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5691 - acc: 0.0000e+00 - val_loss: 1.7420 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "698/698 [==============================] - 0s 46us/step - loss: 1.5730 - acc: 0.0000e+00 - val_loss: 1.7714 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5634 - acc: 0.0000e+00 - val_loss: 1.7132 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5758 - acc: 0.0000e+00 - val_loss: 1.7380 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5709 - acc: 0.0000e+00 - val_loss: 1.7462 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5664 - acc: 0.0000e+00 - val_loss: 1.7314 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5674 - acc: 0.0000e+00 - val_loss: 1.7219 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5648 - acc: 0.0000e+00 - val_loss: 1.7328 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5701 - acc: 0.0000e+00 - val_loss: 1.7389 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5673 - acc: 0.0000e+00 - val_loss: 1.7487 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5705 - acc: 0.0000e+00 - val_loss: 1.7275 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5674 - acc: 0.0000e+00 - val_loss: 1.7178 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5645 - acc: 0.0000e+00 - val_loss: 1.7229 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5761 - acc: 0.0000e+00 - val_loss: 1.7428 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5672 - acc: 0.0000e+00 - val_loss: 1.7309 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5637 - acc: 0.0000e+00 - val_loss: 1.7166 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5615 - acc: 0.0000e+00 - val_loss: 1.7594 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5800 - acc: 0.0000e+00 - val_loss: 1.7392 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5628 - acc: 0.0000e+00 - val_loss: 1.7233 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5635 - acc: 0.0000e+00 - val_loss: 1.7283 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5618 - acc: 0.0000e+00 - val_loss: 1.7500 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5622 - acc: 0.0000e+00 - val_loss: 1.7264 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5686 - acc: 0.0000e+00 - val_loss: 1.7618 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5687 - acc: 0.0000e+00 - val_loss: 1.7226 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5634 - acc: 0.0000e+00 - val_loss: 1.7373 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5620 - acc: 0.0000e+00 - val_loss: 1.7306 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5684 - acc: 0.0000e+00 - val_loss: 1.7094 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5664 - acc: 0.0000e+00 - val_loss: 1.7271 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5614 - acc: 0.0000e+00 - val_loss: 1.7391 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5611 - acc: 0.0000e+00 - val_loss: 1.7261 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5595 - acc: 0.0000e+00 - val_loss: 1.7330 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5633 - acc: 0.0000e+00 - val_loss: 1.7239 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5600 - acc: 0.0000e+00 - val_loss: 1.7113 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5592 - acc: 0.0000e+00 - val_loss: 1.7435 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5678 - acc: 0.0000e+00 - val_loss: 1.7278 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5599 - acc: 0.0000e+00 - val_loss: 1.7179 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5647 - acc: 0.0000e+00 - val_loss: 1.7558 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5620 - acc: 0.0000e+00 - val_loss: 1.7176 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5614 - acc: 0.0000e+00 - val_loss: 1.7444 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5761 - acc: 0.0000e+00 - val_loss: 1.7773 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5730 - acc: 0.0000e+00 - val_loss: 1.7057 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5656 - acc: 0.0000e+00 - val_loss: 1.7317 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5581 - acc: 0.0000e+00 - val_loss: 1.7108 - val_acc: 0.0000e+00\n",
      "Epoch 445/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5583 - acc: 0.0000e+00 - val_loss: 1.7238 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5620 - acc: 0.0000e+00 - val_loss: 1.7109 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5567 - acc: 0.0000e+00 - val_loss: 1.7316 - val_acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5582 - acc: 0.0000e+00 - val_loss: 1.7268 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5605 - acc: 0.0000e+00 - val_loss: 1.7087 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5653 - acc: 0.0000e+00 - val_loss: 1.7748 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5729 - acc: 0.0000e+00 - val_loss: 1.7260 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5595 - acc: 0.0000e+00 - val_loss: 1.7246 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5564 - acc: 0.0000e+00 - val_loss: 1.7337 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5602 - acc: 0.0000e+00 - val_loss: 1.7145 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5558 - acc: 0.0000e+00 - val_loss: 1.7131 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5576 - acc: 0.0000e+00 - val_loss: 1.7239 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5638 - acc: 0.0000e+00 - val_loss: 1.7099 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5643 - acc: 0.0000e+00 - val_loss: 1.7246 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5615 - acc: 0.0000e+00 - val_loss: 1.7035 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5616 - acc: 0.0000e+00 - val_loss: 1.7319 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5564 - acc: 0.0000e+00 - val_loss: 1.7146 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5566 - acc: 0.0000e+00 - val_loss: 1.7131 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5624 - acc: 0.0000e+00 - val_loss: 1.7364 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5550 - acc: 0.0000e+00 - val_loss: 1.7244 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5606 - acc: 0.0000e+00 - val_loss: 1.7295 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5542 - acc: 0.0000e+00 - val_loss: 1.7239 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5599 - acc: 0.0000e+00 - val_loss: 1.7485 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5564 - acc: 0.0000e+00 - val_loss: 1.7096 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5558 - acc: 0.0000e+00 - val_loss: 1.7101 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5473 - acc: 0.0000e+00 - val_loss: 1.7631 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5711 - acc: 0.0000e+00 - val_loss: 1.7089 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5529 - acc: 0.0000e+00 - val_loss: 1.7320 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.5003 - acc: 0.0000e+0 - 0s 30us/step - loss: 1.5699 - acc: 0.0000e+00 - val_loss: 1.7562 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5530 - acc: 0.0000e+00 - val_loss: 1.7125 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5523 - acc: 0.0000e+00 - val_loss: 1.7349 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5645 - acc: 0.0000e+00 - val_loss: 1.7002 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5502 - acc: 0.0000e+00 - val_loss: 1.7285 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5523 - acc: 0.0000e+00 - val_loss: 1.7172 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5555 - acc: 0.0000e+00 - val_loss: 1.7295 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5505 - acc: 0.0000e+00 - val_loss: 1.7173 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5570 - acc: 0.0000e+00 - val_loss: 1.7068 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5524 - acc: 0.0000e+00 - val_loss: 1.7303 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5502 - acc: 0.0000e+00 - val_loss: 1.7004 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5633 - acc: 0.0000e+00 - val_loss: 1.7191 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5514 - acc: 0.0000e+00 - val_loss: 1.7202 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5508 - acc: 0.0000e+00 - val_loss: 1.7203 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5513 - acc: 0.0000e+00 - val_loss: 1.7069 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "698/698 [==============================] - 0s 26us/step - loss: 1.5566 - acc: 0.0000e+00 - val_loss: 1.7195 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5531 - acc: 0.0000e+00 - val_loss: 1.7103 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5646 - acc: 0.0000e+00 - val_loss: 1.7078 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5557 - acc: 0.0000e+00 - val_loss: 1.7232 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5511 - acc: 0.0000e+00 - val_loss: 1.7259 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5571 - acc: 0.0000e+00 - val_loss: 1.7426 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5518 - acc: 0.0000e+00 - val_loss: 1.7016 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5485 - acc: 0.0000e+00 - val_loss: 1.7258 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5531 - acc: 0.0000e+00 - val_loss: 1.7323 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5704 - acc: 0.0000e+00 - val_loss: 1.6963 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "698/698 [==============================] - 0s 28us/step - loss: 1.5593 - acc: 0.0000e+00 - val_loss: 1.6978 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "698/698 [==============================] - 0s 27us/step - loss: 1.5513 - acc: 0.0000e+00 - val_loss: 1.7511 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5580 - acc: 0.0000e+00 - val_loss: 1.6967 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train.values, y = y_train.values, epochs = 500, \n",
    "                    verbose=1, batch_size = 32, shuffle=True, \n",
    "                    validation_data = (X_test.values, y_test.values) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.31132033],\n",
       "        [1.5263102 ],\n",
       "        [0.8499537 ]], dtype=float32), array([2.5805793], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 25us/step\n",
      "\n",
      "Loss = 1.6967269291196552\n",
      "Test Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "preds = model.evaluate(X_test.values, y_test.values)\n",
    "### END CODE HERE ###\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras的Shallow NN\n",
    "\n",
    "Keras的优势是实现各种尝试神经网络，下面就是增加一层后形成了一个Shallow NN的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallowNnModel(input_shape):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    X = Dense(3, activation='relu', name='fc1')(X_input)\n",
    "    X = Dense(1, activation='linear', name='fc2')(X)\n",
    "    \n",
    "    model = Model(inputs = X_input, outputs = X, name='shallowNnModel')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"215pt\" viewBox=\"0.00 0.00 211.51 215.00\" width=\"212pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 211)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-211 207.5107,-211 207.5107,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4665429464 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4665429464</title>\n",
       "<polygon fill=\"none\" points=\"0,-162.5 0,-206.5 203.5107,-206.5 203.5107,-162.5 0,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39.0967\" y=\"-180.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"78.1934,-162.5 78.1934,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.0278\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"78.1934,-184.5 133.8623,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"106.0278\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"133.8623,-162.5 133.8623,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.6865\" y=\"-191.3\">(None, 3)</text>\n",
       "<polyline fill=\"none\" points=\"133.8623,-184.5 203.5107,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"168.6865\" y=\"-169.3\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 4665429408 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4665429408</title>\n",
       "<polygon fill=\"none\" points=\"13.6035,-81.5 13.6035,-125.5 189.9072,-125.5 189.9072,-81.5 13.6035,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39.0967\" y=\"-99.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"64.5898,-81.5 64.5898,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.4243\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64.5898,-103.5 120.2588,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.4243\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120.2588,-81.5 120.2588,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.083\" y=\"-110.3\">(None, 3)</text>\n",
       "<polyline fill=\"none\" points=\"120.2588,-103.5 189.9072,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.083\" y=\"-88.3\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 4665429464&#45;&gt;4665429408 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4665429464-&gt;4665429408</title>\n",
       "<path d=\"M101.7554,-162.3664C101.7554,-154.1516 101.7554,-144.6579 101.7554,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"105.2555,-135.6068 101.7554,-125.6068 98.2555,-135.6069 105.2555,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4665886480 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4665886480</title>\n",
       "<polygon fill=\"none\" points=\"13.6035,-.5 13.6035,-44.5 189.9072,-44.5 189.9072,-.5 13.6035,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"39.0967\" y=\"-18.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"64.5898,-.5 64.5898,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.4243\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"64.5898,-22.5 120.2588,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"92.4243\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"120.2588,-.5 120.2588,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.083\" y=\"-29.3\">(None, 3)</text>\n",
       "<polyline fill=\"none\" points=\"120.2588,-22.5 189.9072,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.083\" y=\"-7.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 4665429408&#45;&gt;4665886480 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4665429408-&gt;4665886480</title>\n",
       "<path d=\"M101.7554,-81.3664C101.7554,-73.1516 101.7554,-63.6579 101.7554,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"105.2555,-54.6068 101.7554,-44.6068 98.2555,-54.6069 105.2555,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = shallowNnModel(X_train.shape[1:])\n",
    "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 698 samples, validate on 175 samples\n",
      "Epoch 1/500\n",
      "698/698 [==============================] - 0s 254us/step - loss: 68.1627 - acc: 0.0000e+00 - val_loss: 63.8648 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 61.7588 - acc: 0.0000e+00 - val_loss: 56.9007 - val_acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 54.0074 - acc: 0.0000e+00 - val_loss: 48.6594 - val_acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 45.5663 - acc: 0.0000e+00 - val_loss: 39.2669 - val_acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 37.3688 - acc: 0.0000e+00 - val_loss: 31.4321 - val_acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 31.0429 - acc: 0.0000e+00 - val_loss: 26.3482 - val_acc: 0.0000e+00\n",
      "Epoch 7/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 27.6190 - acc: 0.0000e+00 - val_loss: 23.6067 - val_acc: 0.0000e+00\n",
      "Epoch 8/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 25.9925 - acc: 0.0000e+00 - val_loss: 22.4060 - val_acc: 0.0000e+00\n",
      "Epoch 9/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 25.1906 - acc: 0.0000e+00 - val_loss: 21.6628 - val_acc: 0.0000e+00\n",
      "Epoch 10/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 24.6411 - acc: 0.0000e+00 - val_loss: 21.1202 - val_acc: 0.0000e+00\n",
      "Epoch 11/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 24.1665 - acc: 0.0000e+00 - val_loss: 20.6275 - val_acc: 0.0000e+00\n",
      "Epoch 12/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 23.7184 - acc: 0.0000e+00 - val_loss: 20.1880 - val_acc: 0.0000e+00\n",
      "Epoch 13/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 23.2700 - acc: 0.0000e+00 - val_loss: 19.7830 - val_acc: 0.0000e+00\n",
      "Epoch 14/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 22.8419 - acc: 0.0000e+00 - val_loss: 19.3581 - val_acc: 0.0000e+00\n",
      "Epoch 15/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 22.4106 - acc: 0.0000e+00 - val_loss: 18.9453 - val_acc: 0.0000e+00\n",
      "Epoch 16/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 21.9835 - acc: 0.0000e+00 - val_loss: 18.5614 - val_acc: 0.0000e+00\n",
      "Epoch 17/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 21.5782 - acc: 0.0000e+00 - val_loss: 18.1409 - val_acc: 0.0000e+00\n",
      "Epoch 18/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 21.1851 - acc: 0.0000e+00 - val_loss: 17.8104 - val_acc: 0.0000e+00\n",
      "Epoch 19/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 20.7729 - acc: 0.0000e+00 - val_loss: 17.3779 - val_acc: 0.0000e+00\n",
      "Epoch 20/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 20.3493 - acc: 0.0000e+00 - val_loss: 17.0438 - val_acc: 0.0000e+00\n",
      "Epoch 21/500\n",
      "698/698 [==============================] - 0s 44us/step - loss: 19.9466 - acc: 0.0000e+00 - val_loss: 16.6713 - val_acc: 0.0000e+00\n",
      "Epoch 22/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 19.5602 - acc: 0.0000e+00 - val_loss: 16.3099 - val_acc: 0.0000e+00\n",
      "Epoch 23/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 19.1824 - acc: 0.0000e+00 - val_loss: 16.0173 - val_acc: 0.0000e+00\n",
      "Epoch 24/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 18.8073 - acc: 0.0000e+00 - val_loss: 15.6892 - val_acc: 0.0000e+00\n",
      "Epoch 25/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 18.4482 - acc: 0.0000e+00 - val_loss: 15.3932 - val_acc: 0.0000e+00\n",
      "Epoch 26/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 18.0761 - acc: 0.0000e+00 - val_loss: 15.0538 - val_acc: 0.0000e+00\n",
      "Epoch 27/500\n",
      "698/698 [==============================] - 0s 45us/step - loss: 17.7363 - acc: 0.0000e+00 - val_loss: 14.7523 - val_acc: 0.0000e+00\n",
      "Epoch 28/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 17.3931 - acc: 0.0000e+00 - val_loss: 14.4520 - val_acc: 0.0000e+00\n",
      "Epoch 29/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 17.0714 - acc: 0.0000e+00 - val_loss: 14.1985 - val_acc: 0.0000e+00\n",
      "Epoch 30/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 16.7402 - acc: 0.0000e+00 - val_loss: 13.8748 - val_acc: 0.0000e+00\n",
      "Epoch 31/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 16.4234 - acc: 0.0000e+00 - val_loss: 13.5976 - val_acc: 0.0000e+00\n",
      "Epoch 32/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 16.1044 - acc: 0.0000e+00 - val_loss: 13.3375 - val_acc: 0.0000e+00\n",
      "Epoch 33/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 15.8272 - acc: 0.0000e+00 - val_loss: 13.1011 - val_acc: 0.0000e+00\n",
      "Epoch 34/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 15.5090 - acc: 0.0000e+00 - val_loss: 12.8253 - val_acc: 0.0000e+00\n",
      "Epoch 35/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 15.2206 - acc: 0.0000e+00 - val_loss: 12.5869 - val_acc: 0.0000e+00\n",
      "Epoch 36/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 14.9308 - acc: 0.0000e+00 - val_loss: 12.3407 - val_acc: 0.0000e+00\n",
      "Epoch 37/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 14.6402 - acc: 0.0000e+00 - val_loss: 12.1263 - val_acc: 0.0000e+00\n",
      "Epoch 38/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 14.3807 - acc: 0.0000e+00 - val_loss: 11.8784 - val_acc: 0.0000e+00\n",
      "Epoch 39/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 14.0914 - acc: 0.0000e+00 - val_loss: 11.6553 - val_acc: 0.0000e+00\n",
      "Epoch 40/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 13.8298 - acc: 0.0000e+00 - val_loss: 11.4264 - val_acc: 0.0000e+00\n",
      "Epoch 41/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 13.5710 - acc: 0.0000e+00 - val_loss: 11.1858 - val_acc: 0.0000e+00\n",
      "Epoch 42/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 13.2874 - acc: 0.0000e+00 - val_loss: 10.9745 - val_acc: 0.0000e+00\n",
      "Epoch 43/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 13.0184 - acc: 0.0000e+00 - val_loss: 10.7436 - val_acc: 0.0000e+00\n",
      "Epoch 44/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 12.7579 - acc: 0.0000e+00 - val_loss: 10.5105 - val_acc: 0.0000e+00\n",
      "Epoch 45/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 12.4883 - acc: 0.0000e+00 - val_loss: 10.3130 - val_acc: 0.0000e+00\n",
      "Epoch 46/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 12.2298 - acc: 0.0000e+00 - val_loss: 10.0931 - val_acc: 0.0000e+00\n",
      "Epoch 47/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 11.9615 - acc: 0.0000e+00 - val_loss: 9.8453 - val_acc: 0.0000e+00\n",
      "Epoch 48/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 11.7008 - acc: 0.0000e+00 - val_loss: 9.6301 - val_acc: 0.0000e+00\n",
      "Epoch 49/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 11.4342 - acc: 0.0000e+00 - val_loss: 9.4080 - val_acc: 0.0000e+00\n",
      "Epoch 50/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 11.1635 - acc: 0.0000e+00 - val_loss: 9.1824 - val_acc: 0.0000e+00\n",
      "Epoch 51/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 10.8930 - acc: 0.0000e+00 - val_loss: 8.9627 - val_acc: 0.0000e+00\n",
      "Epoch 52/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 10.6209 - acc: 0.0000e+00 - val_loss: 8.7272 - val_acc: 0.0000e+00\n",
      "Epoch 53/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 10.3489 - acc: 0.0000e+00 - val_loss: 8.4976 - val_acc: 0.0000e+00\n",
      "Epoch 54/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 10.0603 - acc: 0.0000e+00 - val_loss: 8.2521 - val_acc: 0.0000e+00\n",
      "Epoch 55/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 9.7964 - acc: 0.0000e+00 - val_loss: 8.0353 - val_acc: 0.0000e+00\n",
      "Epoch 56/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 9.4959 - acc: 0.0000e+00 - val_loss: 7.7736 - val_acc: 0.0000e+00\n",
      "Epoch 57/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 0s 37us/step - loss: 9.2503 - acc: 0.0000e+00 - val_loss: 7.5270 - val_acc: 0.0000e+00\n",
      "Epoch 58/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 8.9095 - acc: 0.0000e+00 - val_loss: 7.2956 - val_acc: 0.0000e+00\n",
      "Epoch 59/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 8.6364 - acc: 0.0000e+00 - val_loss: 7.0399 - val_acc: 0.0000e+00\n",
      "Epoch 60/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 8.3182 - acc: 0.0000e+00 - val_loss: 6.7870 - val_acc: 0.0000e+00\n",
      "Epoch 61/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 7.9987 - acc: 0.0000e+00 - val_loss: 6.5301 - val_acc: 0.0000e+00\n",
      "Epoch 62/500\n",
      "698/698 [==============================] - 0s 43us/step - loss: 7.6754 - acc: 0.0000e+00 - val_loss: 6.2711 - val_acc: 0.0000e+00\n",
      "Epoch 63/500\n",
      "698/698 [==============================] - 0s 54us/step - loss: 7.3512 - acc: 0.0000e+00 - val_loss: 6.0033 - val_acc: 0.0000e+00\n",
      "Epoch 64/500\n",
      "698/698 [==============================] - 0s 41us/step - loss: 7.0277 - acc: 0.0000e+00 - val_loss: 5.7256 - val_acc: 0.0000e+00\n",
      "Epoch 65/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 6.7027 - acc: 0.0000e+00 - val_loss: 5.4401 - val_acc: 0.0000e+00\n",
      "Epoch 66/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 6.3366 - acc: 0.0000e+00 - val_loss: 5.1537 - val_acc: 0.0000e+00\n",
      "Epoch 67/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 5.9735 - acc: 0.0000e+00 - val_loss: 4.8608 - val_acc: 0.0000e+00\n",
      "Epoch 68/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 5.6273 - acc: 0.0000e+00 - val_loss: 4.6100 - val_acc: 0.0000e+00\n",
      "Epoch 69/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 5.3060 - acc: 0.0000e+00 - val_loss: 4.2836 - val_acc: 0.0000e+00\n",
      "Epoch 70/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 4.8620 - acc: 0.0000e+00 - val_loss: 3.9434 - val_acc: 0.0000e+00\n",
      "Epoch 71/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 4.4984 - acc: 0.0000e+00 - val_loss: 3.6306 - val_acc: 0.0000e+00\n",
      "Epoch 72/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 4.1057 - acc: 0.0000e+00 - val_loss: 3.3097 - val_acc: 0.0000e+00\n",
      "Epoch 73/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 3.7101 - acc: 0.0000e+00 - val_loss: 3.0049 - val_acc: 0.0000e+00\n",
      "Epoch 74/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 3.3545 - acc: 0.0000e+00 - val_loss: 2.7150 - val_acc: 0.0000e+00\n",
      "Epoch 75/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 2.9427 - acc: 0.0000e+00 - val_loss: 2.4593 - val_acc: 0.0000e+00\n",
      "Epoch 76/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 2.5877 - acc: 0.0000e+00 - val_loss: 2.2357 - val_acc: 0.0000e+00\n",
      "Epoch 77/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 2.2890 - acc: 0.0000e+00 - val_loss: 2.0189 - val_acc: 0.0000e+00\n",
      "Epoch 78/500\n",
      "698/698 [==============================] - 0s 44us/step - loss: 2.0376 - acc: 0.0000e+00 - val_loss: 1.9114 - val_acc: 0.0000e+00\n",
      "Epoch 79/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.8615 - acc: 0.0000e+00 - val_loss: 1.7877 - val_acc: 0.0000e+00\n",
      "Epoch 80/500\n",
      "698/698 [==============================] - 0s 48us/step - loss: 1.7181 - acc: 0.0000e+00 - val_loss: 1.9158 - val_acc: 0.0000e+00\n",
      "Epoch 81/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.6949 - acc: 0.0000e+00 - val_loss: 1.7684 - val_acc: 0.0000e+00\n",
      "Epoch 82/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6437 - acc: 0.0000e+00 - val_loss: 1.7498 - val_acc: 0.0000e+00\n",
      "Epoch 83/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6435 - acc: 0.0000e+00 - val_loss: 1.7860 - val_acc: 0.0000e+00\n",
      "Epoch 84/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6237 - acc: 0.0000e+00 - val_loss: 1.8273 - val_acc: 0.0000e+00\n",
      "Epoch 85/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6338 - acc: 0.0000e+00 - val_loss: 1.8415 - val_acc: 0.0000e+00\n",
      "Epoch 86/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.6200 - acc: 0.0000e+00 - val_loss: 1.7633 - val_acc: 0.0000e+00\n",
      "Epoch 87/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6142 - acc: 0.0000e+00 - val_loss: 1.7607 - val_acc: 0.0000e+00\n",
      "Epoch 88/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6090 - acc: 0.0000e+00 - val_loss: 1.7578 - val_acc: 0.0000e+00\n",
      "Epoch 89/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.6169 - acc: 0.0000e+00 - val_loss: 1.7632 - val_acc: 0.0000e+00\n",
      "Epoch 90/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6199 - acc: 0.0000e+00 - val_loss: 1.7603 - val_acc: 0.0000e+00\n",
      "Epoch 91/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6349 - acc: 0.0000e+00 - val_loss: 1.7641 - val_acc: 0.0000e+00\n",
      "Epoch 92/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6133 - acc: 0.0000e+00 - val_loss: 1.7569 - val_acc: 0.0000e+00\n",
      "Epoch 93/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6110 - acc: 0.0000e+00 - val_loss: 1.7570 - val_acc: 0.0000e+00\n",
      "Epoch 94/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6292 - acc: 0.0000e+00 - val_loss: 1.7717 - val_acc: 0.0000e+00\n",
      "Epoch 95/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6404 - acc: 0.0000e+00 - val_loss: 1.7717 - val_acc: 0.0000e+00\n",
      "Epoch 96/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6038 - acc: 0.0000e+00 - val_loss: 1.7572 - val_acc: 0.0000e+00\n",
      "Epoch 97/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.6185 - acc: 0.0000e+00 - val_loss: 1.7739 - val_acc: 0.0000e+00\n",
      "Epoch 98/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6420 - acc: 0.0000e+00 - val_loss: 1.8289 - val_acc: 0.0000e+00\n",
      "Epoch 99/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6230 - acc: 0.0000e+00 - val_loss: 1.8590 - val_acc: 0.0000e+00\n",
      "Epoch 100/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6149 - acc: 0.0000e+00 - val_loss: 1.7519 - val_acc: 0.0000e+00\n",
      "Epoch 101/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.6061 - acc: 0.0000e+00 - val_loss: 1.7992 - val_acc: 0.0000e+00\n",
      "Epoch 102/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6193 - acc: 0.0000e+00 - val_loss: 1.8063 - val_acc: 0.0000e+00\n",
      "Epoch 103/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.6047 - acc: 0.0000e+00 - val_loss: 1.7728 - val_acc: 0.0000e+00\n",
      "Epoch 104/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6049 - acc: 0.0000e+00 - val_loss: 1.7646 - val_acc: 0.0000e+00\n",
      "Epoch 105/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6207 - acc: 0.0000e+00 - val_loss: 1.7924 - val_acc: 0.0000e+00\n",
      "Epoch 106/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6119 - acc: 0.0000e+00 - val_loss: 1.7518 - val_acc: 0.0000e+00\n",
      "Epoch 107/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6211 - acc: 0.0000e+00 - val_loss: 1.7594 - val_acc: 0.0000e+00\n",
      "Epoch 108/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6205 - acc: 0.0000e+00 - val_loss: 1.7645 - val_acc: 0.0000e+00\n",
      "Epoch 109/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.6159 - acc: 0.0000e+00 - val_loss: 1.7446 - val_acc: 0.0000e+00\n",
      "Epoch 110/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6064 - acc: 0.0000e+00 - val_loss: 1.7428 - val_acc: 0.0000e+00\n",
      "Epoch 111/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.6189 - acc: 0.0000e+00 - val_loss: 1.7535 - val_acc: 0.0000e+00\n",
      "Epoch 112/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5981 - acc: 0.0000e+00 - val_loss: 1.8167 - val_acc: 0.0000e+00\n",
      "Epoch 113/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6030 - acc: 0.0000e+00 - val_loss: 1.7612 - val_acc: 0.0000e+00\n",
      "Epoch 114/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 0s 33us/step - loss: 1.6036 - acc: 0.0000e+00 - val_loss: 1.7506 - val_acc: 0.0000e+00\n",
      "Epoch 115/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5955 - acc: 0.0000e+00 - val_loss: 1.7388 - val_acc: 0.0000e+00\n",
      "Epoch 116/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5979 - acc: 0.0000e+00 - val_loss: 1.7510 - val_acc: 0.0000e+00\n",
      "Epoch 117/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5915 - acc: 0.0000e+00 - val_loss: 1.8498 - val_acc: 0.0000e+00\n",
      "Epoch 118/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.6198 - acc: 0.0000e+00 - val_loss: 1.7426 - val_acc: 0.0000e+00\n",
      "Epoch 119/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5987 - acc: 0.0000e+00 - val_loss: 1.7917 - val_acc: 0.0000e+00\n",
      "Epoch 120/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.6122 - acc: 0.0000e+00 - val_loss: 1.7930 - val_acc: 0.0000e+00\n",
      "Epoch 121/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5930 - acc: 0.0000e+00 - val_loss: 1.7374 - val_acc: 0.0000e+00\n",
      "Epoch 122/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5899 - acc: 0.0000e+00 - val_loss: 1.8389 - val_acc: 0.0000e+00\n",
      "Epoch 123/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.6267 - acc: 0.0000e+00 - val_loss: 1.7451 - val_acc: 0.0000e+00\n",
      "Epoch 124/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5925 - acc: 0.0000e+00 - val_loss: 1.7409 - val_acc: 0.0000e+00\n",
      "Epoch 125/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5910 - acc: 0.0000e+00 - val_loss: 1.7578 - val_acc: 0.0000e+00\n",
      "Epoch 126/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5913 - acc: 0.0000e+00 - val_loss: 1.7384 - val_acc: 0.0000e+00\n",
      "Epoch 127/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6195 - acc: 0.0000e+00 - val_loss: 1.8234 - val_acc: 0.0000e+00\n",
      "Epoch 128/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5863 - acc: 0.0000e+00 - val_loss: 1.7635 - val_acc: 0.0000e+00\n",
      "Epoch 129/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5943 - acc: 0.0000e+00 - val_loss: 1.7642 - val_acc: 0.0000e+00\n",
      "Epoch 130/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5760 - acc: 0.0000e+00 - val_loss: 1.7458 - val_acc: 0.0000e+00\n",
      "Epoch 131/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5990 - acc: 0.0000e+00 - val_loss: 1.7367 - val_acc: 0.0000e+00\n",
      "Epoch 132/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6260 - acc: 0.0000e+00 - val_loss: 1.7542 - val_acc: 0.0000e+00\n",
      "Epoch 133/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5895 - acc: 0.0000e+00 - val_loss: 1.7702 - val_acc: 0.0000e+00\n",
      "Epoch 134/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5950 - acc: 0.0000e+00 - val_loss: 1.7849 - val_acc: 0.0000e+00\n",
      "Epoch 135/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5915 - acc: 0.0000e+00 - val_loss: 1.7657 - val_acc: 0.0000e+00\n",
      "Epoch 136/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6027 - acc: 0.0000e+00 - val_loss: 1.7320 - val_acc: 0.0000e+00\n",
      "Epoch 137/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5842 - acc: 0.0000e+00 - val_loss: 1.7532 - val_acc: 0.0000e+00\n",
      "Epoch 138/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5909 - acc: 0.0000e+00 - val_loss: 1.7549 - val_acc: 0.0000e+00\n",
      "Epoch 139/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5856 - acc: 0.0000e+00 - val_loss: 1.7291 - val_acc: 0.0000e+00\n",
      "Epoch 140/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5851 - acc: 0.0000e+00 - val_loss: 1.7287 - val_acc: 0.0000e+00\n",
      "Epoch 141/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5781 - acc: 0.0000e+00 - val_loss: 1.8236 - val_acc: 0.0000e+00\n",
      "Epoch 142/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.6205 - acc: 0.0000e+00 - val_loss: 1.7477 - val_acc: 0.0000e+00\n",
      "Epoch 143/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5784 - acc: 0.0000e+00 - val_loss: 1.7701 - val_acc: 0.0000e+00\n",
      "Epoch 144/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5820 - acc: 0.0000e+00 - val_loss: 1.7294 - val_acc: 0.0000e+00\n",
      "Epoch 145/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5839 - acc: 0.0000e+00 - val_loss: 1.8042 - val_acc: 0.0000e+00\n",
      "Epoch 146/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5984 - acc: 0.0000e+00 - val_loss: 1.7910 - val_acc: 0.0000e+00\n",
      "Epoch 147/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5867 - acc: 0.0000e+00 - val_loss: 1.7182 - val_acc: 0.0000e+00\n",
      "Epoch 148/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5870 - acc: 0.0000e+00 - val_loss: 1.7368 - val_acc: 0.0000e+00\n",
      "Epoch 149/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5883 - acc: 0.0000e+00 - val_loss: 1.7390 - val_acc: 0.0000e+00\n",
      "Epoch 150/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5882 - acc: 0.0000e+00 - val_loss: 1.7303 - val_acc: 0.0000e+00\n",
      "Epoch 151/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5765 - acc: 0.0000e+00 - val_loss: 1.7646 - val_acc: 0.0000e+00\n",
      "Epoch 152/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5762 - acc: 0.0000e+00 - val_loss: 1.7173 - val_acc: 0.0000e+00\n",
      "Epoch 153/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5779 - acc: 0.0000e+00 - val_loss: 1.7289 - val_acc: 0.0000e+00\n",
      "Epoch 154/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5819 - acc: 0.0000e+00 - val_loss: 1.7626 - val_acc: 0.0000e+00\n",
      "Epoch 155/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5732 - acc: 0.0000e+00 - val_loss: 1.7348 - val_acc: 0.0000e+00\n",
      "Epoch 156/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5716 - acc: 0.0000e+00 - val_loss: 1.7510 - val_acc: 0.0000e+00\n",
      "Epoch 157/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.6059 - acc: 0.0000e+00 - val_loss: 1.7423 - val_acc: 0.0000e+00\n",
      "Epoch 158/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.6022 - acc: 0.0000e+00 - val_loss: 1.7755 - val_acc: 0.0000e+00\n",
      "Epoch 159/500\n",
      "698/698 [==============================] - 0s 44us/step - loss: 1.5710 - acc: 0.0000e+00 - val_loss: 1.7378 - val_acc: 0.0000e+00\n",
      "Epoch 160/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 1.5658 - acc: 0.0000e+00 - val_loss: 1.7151 - val_acc: 0.0000e+00\n",
      "Epoch 161/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5795 - acc: 0.0000e+00 - val_loss: 1.7223 - val_acc: 0.0000e+00\n",
      "Epoch 162/500\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.4990 - acc: 0.0000e+0 - 0s 44us/step - loss: 1.5721 - acc: 0.0000e+00 - val_loss: 1.7244 - val_acc: 0.0000e+00\n",
      "Epoch 163/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5872 - acc: 0.0000e+00 - val_loss: 1.7135 - val_acc: 0.0000e+00\n",
      "Epoch 164/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 1.5671 - acc: 0.0000e+00 - val_loss: 1.7285 - val_acc: 0.0000e+00\n",
      "Epoch 165/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 1.5718 - acc: 0.0000e+00 - val_loss: 1.7125 - val_acc: 0.0000e+00\n",
      "Epoch 166/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.6015 - acc: 0.0000e+00 - val_loss: 1.7251 - val_acc: 0.0000e+00\n",
      "Epoch 167/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5617 - acc: 0.0000e+00 - val_loss: 1.7204 - val_acc: 0.0000e+00\n",
      "Epoch 168/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5663 - acc: 0.0000e+00 - val_loss: 1.7243 - val_acc: 0.0000e+00\n",
      "Epoch 169/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5693 - acc: 0.0000e+00 - val_loss: 1.7494 - val_acc: 0.0000e+00\n",
      "Epoch 170/500\n",
      "698/698 [==============================] - 0s 43us/step - loss: 1.5783 - acc: 0.0000e+00 - val_loss: 1.7228 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 1.5664 - acc: 0.0000e+00 - val_loss: 1.7392 - val_acc: 0.0000e+00\n",
      "Epoch 172/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5728 - acc: 0.0000e+00 - val_loss: 1.7478 - val_acc: 0.0000e+00\n",
      "Epoch 173/500\n",
      "698/698 [==============================] - 0s 43us/step - loss: 1.5698 - acc: 0.0000e+00 - val_loss: 1.7266 - val_acc: 0.0000e+00\n",
      "Epoch 174/500\n",
      "698/698 [==============================] - 0s 44us/step - loss: 1.5671 - acc: 0.0000e+00 - val_loss: 1.7236 - val_acc: 0.0000e+00\n",
      "Epoch 175/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5865 - acc: 0.0000e+00 - val_loss: 1.7277 - val_acc: 0.0000e+00\n",
      "Epoch 176/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5737 - acc: 0.0000e+00 - val_loss: 1.7211 - val_acc: 0.0000e+00\n",
      "Epoch 177/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5752 - acc: 0.0000e+00 - val_loss: 1.7147 - val_acc: 0.0000e+00\n",
      "Epoch 178/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5617 - acc: 0.0000e+00 - val_loss: 1.7033 - val_acc: 0.0000e+00\n",
      "Epoch 179/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5601 - acc: 0.0000e+00 - val_loss: 1.7249 - val_acc: 0.0000e+00\n",
      "Epoch 180/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5630 - acc: 0.0000e+00 - val_loss: 1.7092 - val_acc: 0.0000e+00\n",
      "Epoch 181/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5796 - acc: 0.0000e+00 - val_loss: 1.7139 - val_acc: 0.0000e+00\n",
      "Epoch 182/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5989 - acc: 0.0000e+00 - val_loss: 1.7243 - val_acc: 0.0000e+00\n",
      "Epoch 183/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5874 - acc: 0.0000e+00 - val_loss: 1.7867 - val_acc: 0.0000e+00\n",
      "Epoch 184/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5729 - acc: 0.0000e+00 - val_loss: 1.7678 - val_acc: 0.0000e+00\n",
      "Epoch 185/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5602 - acc: 0.0000e+00 - val_loss: 1.7457 - val_acc: 0.0000e+00\n",
      "Epoch 186/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5583 - acc: 0.0000e+00 - val_loss: 1.7413 - val_acc: 0.0000e+00\n",
      "Epoch 187/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5712 - acc: 0.0000e+00 - val_loss: 1.7113 - val_acc: 0.0000e+00\n",
      "Epoch 188/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5679 - acc: 0.0000e+00 - val_loss: 1.7307 - val_acc: 0.0000e+00\n",
      "Epoch 189/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5566 - acc: 0.0000e+00 - val_loss: 1.7946 - val_acc: 0.0000e+00\n",
      "Epoch 190/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5713 - acc: 0.0000e+00 - val_loss: 1.7024 - val_acc: 0.0000e+00\n",
      "Epoch 191/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5580 - acc: 0.0000e+00 - val_loss: 1.7093 - val_acc: 0.0000e+00\n",
      "Epoch 192/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5669 - acc: 0.0000e+00 - val_loss: 1.7426 - val_acc: 0.0000e+00\n",
      "Epoch 193/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5685 - acc: 0.0000e+00 - val_loss: 1.7257 - val_acc: 0.0000e+00\n",
      "Epoch 194/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5503 - acc: 0.0000e+00 - val_loss: 1.7373 - val_acc: 0.0000e+00\n",
      "Epoch 195/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5549 - acc: 0.0000e+00 - val_loss: 1.6982 - val_acc: 0.0000e+00\n",
      "Epoch 196/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5778 - acc: 0.0000e+00 - val_loss: 1.7100 - val_acc: 0.0000e+00\n",
      "Epoch 197/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5631 - acc: 0.0000e+00 - val_loss: 1.7611 - val_acc: 0.0000e+00\n",
      "Epoch 198/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5569 - acc: 0.0000e+00 - val_loss: 1.8044 - val_acc: 0.0000e+00\n",
      "Epoch 199/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5570 - acc: 0.0000e+00 - val_loss: 1.8066 - val_acc: 0.0000e+00\n",
      "Epoch 200/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5535 - acc: 0.0000e+00 - val_loss: 1.7185 - val_acc: 0.0000e+00\n",
      "Epoch 201/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5597 - acc: 0.0000e+00 - val_loss: 1.7006 - val_acc: 0.0000e+00\n",
      "Epoch 202/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5596 - acc: 0.0000e+00 - val_loss: 1.8025 - val_acc: 0.0000e+00\n",
      "Epoch 203/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.6278 - acc: 0.0000e+00 - val_loss: 1.7474 - val_acc: 0.0000e+00\n",
      "Epoch 204/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5404 - acc: 0.0000e+00 - val_loss: 1.6978 - val_acc: 0.0000e+00\n",
      "Epoch 205/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5475 - acc: 0.0000e+00 - val_loss: 1.7433 - val_acc: 0.0000e+00\n",
      "Epoch 206/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5731 - acc: 0.0000e+00 - val_loss: 1.7060 - val_acc: 0.0000e+00\n",
      "Epoch 207/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5662 - acc: 0.0000e+00 - val_loss: 1.6914 - val_acc: 0.0000e+00\n",
      "Epoch 208/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5769 - acc: 0.0000e+00 - val_loss: 1.7141 - val_acc: 0.0000e+00\n",
      "Epoch 209/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5585 - acc: 0.0000e+00 - val_loss: 1.7152 - val_acc: 0.0000e+00\n",
      "Epoch 210/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5899 - acc: 0.0000e+00 - val_loss: 1.6949 - val_acc: 0.0000e+00\n",
      "Epoch 211/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5421 - acc: 0.0000e+00 - val_loss: 1.7576 - val_acc: 0.0000e+00\n",
      "Epoch 212/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5580 - acc: 0.0000e+00 - val_loss: 1.6930 - val_acc: 0.0000e+00\n",
      "Epoch 213/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5529 - acc: 0.0000e+00 - val_loss: 1.7324 - val_acc: 0.0000e+00\n",
      "Epoch 214/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5484 - acc: 0.0000e+00 - val_loss: 1.7465 - val_acc: 0.0000e+00\n",
      "Epoch 215/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5492 - acc: 0.0000e+00 - val_loss: 1.7126 - val_acc: 0.0000e+00\n",
      "Epoch 216/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5657 - acc: 0.0000e+00 - val_loss: 1.7210 - val_acc: 0.0000e+00\n",
      "Epoch 217/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5527 - acc: 0.0000e+00 - val_loss: 1.7117 - val_acc: 0.0000e+00\n",
      "Epoch 218/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5419 - acc: 0.0000e+00 - val_loss: 1.7426 - val_acc: 0.0000e+00\n",
      "Epoch 219/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5586 - acc: 0.0000e+00 - val_loss: 1.7348 - val_acc: 0.0000e+00\n",
      "Epoch 220/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5696 - acc: 0.0000e+00 - val_loss: 1.7249 - val_acc: 0.0000e+00\n",
      "Epoch 221/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5589 - acc: 0.0000e+00 - val_loss: 1.7200 - val_acc: 0.0000e+00\n",
      "Epoch 222/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5512 - acc: 0.0000e+00 - val_loss: 1.6863 - val_acc: 0.0000e+00\n",
      "Epoch 223/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5800 - acc: 0.0000e+00 - val_loss: 1.7796 - val_acc: 0.0000e+00\n",
      "Epoch 224/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5504 - acc: 0.0000e+00 - val_loss: 1.7123 - val_acc: 0.0000e+00\n",
      "Epoch 225/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5469 - acc: 0.0000e+00 - val_loss: 1.6872 - val_acc: 0.0000e+00\n",
      "Epoch 226/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5554 - acc: 0.0000e+00 - val_loss: 1.6926 - val_acc: 0.0000e+00\n",
      "Epoch 227/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5547 - acc: 0.0000e+00 - val_loss: 1.7336 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5475 - acc: 0.0000e+00 - val_loss: 1.8004 - val_acc: 0.0000e+00\n",
      "Epoch 229/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5638 - acc: 0.0000e+00 - val_loss: 1.6898 - val_acc: 0.0000e+00\n",
      "Epoch 230/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5464 - acc: 0.0000e+00 - val_loss: 1.7217 - val_acc: 0.0000e+00\n",
      "Epoch 231/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5418 - acc: 0.0000e+00 - val_loss: 1.6884 - val_acc: 0.0000e+00\n",
      "Epoch 232/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5591 - acc: 0.0000e+00 - val_loss: 1.7605 - val_acc: 0.0000e+00\n",
      "Epoch 233/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5497 - acc: 0.0000e+00 - val_loss: 1.7236 - val_acc: 0.0000e+00\n",
      "Epoch 234/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5292 - acc: 0.0000e+00 - val_loss: 1.7598 - val_acc: 0.0000e+00\n",
      "Epoch 235/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5618 - acc: 0.0000e+00 - val_loss: 1.6954 - val_acc: 0.0000e+00\n",
      "Epoch 236/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5490 - acc: 0.0000e+00 - val_loss: 1.6842 - val_acc: 0.0000e+00\n",
      "Epoch 237/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5609 - acc: 0.0000e+00 - val_loss: 1.6897 - val_acc: 0.0000e+00\n",
      "Epoch 238/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5446 - acc: 0.0000e+00 - val_loss: 1.7167 - val_acc: 0.0000e+00\n",
      "Epoch 239/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5588 - acc: 0.0000e+00 - val_loss: 1.8120 - val_acc: 0.0000e+00\n",
      "Epoch 240/500\n",
      "698/698 [==============================] - 0s 43us/step - loss: 1.5549 - acc: 0.0000e+00 - val_loss: 1.7561 - val_acc: 0.0000e+00\n",
      "Epoch 241/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5467 - acc: 0.0000e+00 - val_loss: 1.6898 - val_acc: 0.0000e+00\n",
      "Epoch 242/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5818 - acc: 0.0000e+00 - val_loss: 1.7541 - val_acc: 0.0000e+00\n",
      "Epoch 243/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5284 - acc: 0.0000e+00 - val_loss: 1.7057 - val_acc: 0.0000e+00\n",
      "Epoch 244/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5310 - acc: 0.0000e+00 - val_loss: 1.6900 - val_acc: 0.0000e+00\n",
      "Epoch 245/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5289 - acc: 0.0000e+00 - val_loss: 1.7236 - val_acc: 0.0000e+00\n",
      "Epoch 246/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5729 - acc: 0.0000e+00 - val_loss: 1.8762 - val_acc: 0.0000e+00\n",
      "Epoch 247/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5746 - acc: 0.0000e+00 - val_loss: 1.7002 - val_acc: 0.0000e+00\n",
      "Epoch 248/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5575 - acc: 0.0000e+00 - val_loss: 1.7035 - val_acc: 0.0000e+00\n",
      "Epoch 249/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5484 - acc: 0.0000e+00 - val_loss: 1.6865 - val_acc: 0.0000e+00\n",
      "Epoch 250/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5439 - acc: 0.0000e+00 - val_loss: 1.7000 - val_acc: 0.0000e+00\n",
      "Epoch 251/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5389 - acc: 0.0000e+00 - val_loss: 1.6925 - val_acc: 0.0000e+00\n",
      "Epoch 252/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5483 - acc: 0.0000e+00 - val_loss: 1.6833 - val_acc: 0.0000e+00\n",
      "Epoch 253/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5415 - acc: 0.0000e+00 - val_loss: 1.6848 - val_acc: 0.0000e+00\n",
      "Epoch 254/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5351 - acc: 0.0000e+00 - val_loss: 1.6864 - val_acc: 0.0000e+00\n",
      "Epoch 255/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5302 - acc: 0.0000e+00 - val_loss: 1.7023 - val_acc: 0.0000e+00\n",
      "Epoch 256/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5541 - acc: 0.0000e+00 - val_loss: 1.6794 - val_acc: 0.0000e+00\n",
      "Epoch 257/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5301 - acc: 0.0000e+00 - val_loss: 1.7042 - val_acc: 0.0000e+00\n",
      "Epoch 258/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.5346 - acc: 0.0000e+00 - val_loss: 1.6792 - val_acc: 0.0000e+00\n",
      "Epoch 259/500\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.7645 - acc: 0.0000e+0 - 0s 38us/step - loss: 1.5373 - acc: 0.0000e+00 - val_loss: 1.6877 - val_acc: 0.0000e+00\n",
      "Epoch 260/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5379 - acc: 0.0000e+00 - val_loss: 1.6824 - val_acc: 0.0000e+00\n",
      "Epoch 261/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5436 - acc: 0.0000e+00 - val_loss: 1.7057 - val_acc: 0.0000e+00\n",
      "Epoch 262/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5277 - acc: 0.0000e+00 - val_loss: 1.7424 - val_acc: 0.0000e+00\n",
      "Epoch 263/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5336 - acc: 0.0000e+00 - val_loss: 1.6823 - val_acc: 0.0000e+00\n",
      "Epoch 264/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5274 - acc: 0.0000e+00 - val_loss: 1.7324 - val_acc: 0.0000e+00\n",
      "Epoch 265/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5324 - acc: 0.0000e+00 - val_loss: 1.6955 - val_acc: 0.0000e+00\n",
      "Epoch 266/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5240 - acc: 0.0000e+00 - val_loss: 1.7252 - val_acc: 0.0000e+00\n",
      "Epoch 267/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5230 - acc: 0.0000e+00 - val_loss: 1.7636 - val_acc: 0.0000e+00\n",
      "Epoch 268/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5399 - acc: 0.0000e+00 - val_loss: 1.7631 - val_acc: 0.0000e+00\n",
      "Epoch 269/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5322 - acc: 0.0000e+00 - val_loss: 1.6739 - val_acc: 0.0000e+00\n",
      "Epoch 270/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5903 - acc: 0.0000e+00 - val_loss: 1.7191 - val_acc: 0.0000e+00\n",
      "Epoch 271/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5574 - acc: 0.0000e+00 - val_loss: 1.7025 - val_acc: 0.0000e+00\n",
      "Epoch 272/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5251 - acc: 0.0000e+00 - val_loss: 1.6872 - val_acc: 0.0000e+00\n",
      "Epoch 273/500\n",
      "698/698 [==============================] - 0s 42us/step - loss: 1.5355 - acc: 0.0000e+00 - val_loss: 1.7310 - val_acc: 0.0000e+00\n",
      "Epoch 274/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5747 - acc: 0.0000e+00 - val_loss: 1.7558 - val_acc: 0.0000e+00\n",
      "Epoch 275/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5186 - acc: 0.0000e+00 - val_loss: 1.6885 - val_acc: 0.0000e+00\n",
      "Epoch 276/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5357 - acc: 0.0000e+00 - val_loss: 1.6926 - val_acc: 0.0000e+00\n",
      "Epoch 277/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5391 - acc: 0.0000e+00 - val_loss: 1.6729 - val_acc: 0.0000e+00\n",
      "Epoch 278/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5300 - acc: 0.0000e+00 - val_loss: 1.6767 - val_acc: 0.0000e+00\n",
      "Epoch 279/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5472 - acc: 0.0000e+00 - val_loss: 1.7373 - val_acc: 0.0000e+00\n",
      "Epoch 280/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5241 - acc: 0.0000e+00 - val_loss: 1.7423 - val_acc: 0.0000e+00\n",
      "Epoch 281/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5213 - acc: 0.0000e+00 - val_loss: 1.7264 - val_acc: 0.0000e+00\n",
      "Epoch 282/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5468 - acc: 0.0000e+00 - val_loss: 1.7226 - val_acc: 0.0000e+00\n",
      "Epoch 283/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5213 - acc: 0.0000e+00 - val_loss: 1.7126 - val_acc: 0.0000e+00\n",
      "Epoch 284/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 0s 32us/step - loss: 1.5332 - acc: 0.0000e+00 - val_loss: 1.6990 - val_acc: 0.0000e+00\n",
      "Epoch 285/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5223 - acc: 0.0000e+00 - val_loss: 1.6798 - val_acc: 0.0000e+00\n",
      "Epoch 286/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5358 - acc: 0.0000e+00 - val_loss: 1.6895 - val_acc: 0.0000e+00\n",
      "Epoch 287/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5699 - acc: 0.0000e+00 - val_loss: 1.7788 - val_acc: 0.0000e+00\n",
      "Epoch 288/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5394 - acc: 0.0000e+00 - val_loss: 1.6878 - val_acc: 0.0000e+00\n",
      "Epoch 289/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5257 - acc: 0.0000e+00 - val_loss: 1.6818 - val_acc: 0.0000e+00\n",
      "Epoch 290/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5263 - acc: 0.0000e+00 - val_loss: 1.6818 - val_acc: 0.0000e+00\n",
      "Epoch 291/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5572 - acc: 0.0000e+00 - val_loss: 1.6973 - val_acc: 0.0000e+00\n",
      "Epoch 292/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5204 - acc: 0.0000e+00 - val_loss: 1.6797 - val_acc: 0.0000e+00\n",
      "Epoch 293/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5154 - acc: 0.0000e+00 - val_loss: 1.6694 - val_acc: 0.0000e+00\n",
      "Epoch 294/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5237 - acc: 0.0000e+00 - val_loss: 1.7075 - val_acc: 0.0000e+00\n",
      "Epoch 295/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5230 - acc: 0.0000e+00 - val_loss: 1.6994 - val_acc: 0.0000e+00\n",
      "Epoch 296/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5311 - acc: 0.0000e+00 - val_loss: 1.7432 - val_acc: 0.0000e+00\n",
      "Epoch 297/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5309 - acc: 0.0000e+00 - val_loss: 1.7529 - val_acc: 0.0000e+00\n",
      "Epoch 298/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5664 - acc: 0.0000e+00 - val_loss: 1.6738 - val_acc: 0.0000e+00\n",
      "Epoch 299/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5199 - acc: 0.0000e+00 - val_loss: 1.6721 - val_acc: 0.0000e+00\n",
      "Epoch 300/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5234 - acc: 0.0000e+00 - val_loss: 1.6759 - val_acc: 0.0000e+00\n",
      "Epoch 301/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5317 - acc: 0.0000e+00 - val_loss: 1.6829 - val_acc: 0.0000e+00\n",
      "Epoch 302/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5324 - acc: 0.0000e+00 - val_loss: 1.6800 - val_acc: 0.0000e+00\n",
      "Epoch 303/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5221 - acc: 0.0000e+00 - val_loss: 1.6797 - val_acc: 0.0000e+00\n",
      "Epoch 304/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5393 - acc: 0.0000e+00 - val_loss: 1.6726 - val_acc: 0.0000e+00\n",
      "Epoch 305/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5222 - acc: 0.0000e+00 - val_loss: 1.6847 - val_acc: 0.0000e+00\n",
      "Epoch 306/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5398 - acc: 0.0000e+00 - val_loss: 1.6994 - val_acc: 0.0000e+00\n",
      "Epoch 307/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5267 - acc: 0.0000e+00 - val_loss: 1.6695 - val_acc: 0.0000e+00\n",
      "Epoch 308/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5223 - acc: 0.0000e+00 - val_loss: 1.6705 - val_acc: 0.0000e+00\n",
      "Epoch 309/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5319 - acc: 0.0000e+00 - val_loss: 1.7425 - val_acc: 0.0000e+00\n",
      "Epoch 310/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5441 - acc: 0.0000e+00 - val_loss: 1.7428 - val_acc: 0.0000e+00\n",
      "Epoch 311/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5520 - acc: 0.0000e+00 - val_loss: 1.6887 - val_acc: 0.0000e+00\n",
      "Epoch 312/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5425 - acc: 0.0000e+00 - val_loss: 1.7142 - val_acc: 0.0000e+00\n",
      "Epoch 313/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5276 - acc: 0.0000e+00 - val_loss: 1.7085 - val_acc: 0.0000e+00\n",
      "Epoch 314/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5183 - acc: 0.0000e+00 - val_loss: 1.6695 - val_acc: 0.0000e+00\n",
      "Epoch 315/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5305 - acc: 0.0000e+00 - val_loss: 1.6997 - val_acc: 0.0000e+00\n",
      "Epoch 316/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5266 - acc: 0.0000e+00 - val_loss: 1.7286 - val_acc: 0.0000e+00\n",
      "Epoch 317/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5198 - acc: 0.0000e+00 - val_loss: 1.6657 - val_acc: 0.0000e+00\n",
      "Epoch 318/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5309 - acc: 0.0000e+00 - val_loss: 1.6681 - val_acc: 0.0000e+00\n",
      "Epoch 319/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5237 - acc: 0.0000e+00 - val_loss: 1.6642 - val_acc: 0.0000e+00\n",
      "Epoch 320/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5132 - acc: 0.0000e+00 - val_loss: 1.6974 - val_acc: 0.0000e+00\n",
      "Epoch 321/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5273 - acc: 0.0000e+00 - val_loss: 1.6650 - val_acc: 0.0000e+00\n",
      "Epoch 322/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5138 - acc: 0.0000e+00 - val_loss: 1.6675 - val_acc: 0.0000e+00\n",
      "Epoch 323/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5245 - acc: 0.0000e+00 - val_loss: 1.6654 - val_acc: 0.0000e+00\n",
      "Epoch 324/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5176 - acc: 0.0000e+00 - val_loss: 1.6678 - val_acc: 0.0000e+00\n",
      "Epoch 325/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5230 - acc: 0.0000e+00 - val_loss: 1.7047 - val_acc: 0.0000e+00\n",
      "Epoch 326/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5399 - acc: 0.0000e+00 - val_loss: 1.7066 - val_acc: 0.0000e+00\n",
      "Epoch 327/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5144 - acc: 0.0000e+00 - val_loss: 1.6786 - val_acc: 0.0000e+00\n",
      "Epoch 328/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5369 - acc: 0.0000e+00 - val_loss: 1.6818 - val_acc: 0.0000e+00\n",
      "Epoch 329/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5312 - acc: 0.0000e+00 - val_loss: 1.6784 - val_acc: 0.0000e+00\n",
      "Epoch 330/500\n",
      "698/698 [==============================] - 0s 55us/step - loss: 1.5239 - acc: 0.0000e+00 - val_loss: 1.6697 - val_acc: 0.0000e+00\n",
      "Epoch 331/500\n",
      "698/698 [==============================] - 0s 116us/step - loss: 1.5269 - acc: 0.0000e+00 - val_loss: 1.6770 - val_acc: 0.0000e+00\n",
      "Epoch 332/500\n",
      "698/698 [==============================] - 0s 59us/step - loss: 1.5442 - acc: 0.0000e+00 - val_loss: 1.6885 - val_acc: 0.0000e+00\n",
      "Epoch 333/500\n",
      "698/698 [==============================] - 0s 49us/step - loss: 1.5134 - acc: 0.0000e+00 - val_loss: 1.7572 - val_acc: 0.0000e+00\n",
      "Epoch 334/500\n",
      "698/698 [==============================] - 0s 41us/step - loss: 1.5297 - acc: 0.0000e+00 - val_loss: 1.7202 - val_acc: 0.0000e+00\n",
      "Epoch 335/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5160 - acc: 0.0000e+00 - val_loss: 1.6602 - val_acc: 0.0000e+00\n",
      "Epoch 336/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5341 - acc: 0.0000e+00 - val_loss: 1.6726 - val_acc: 0.0000e+00\n",
      "Epoch 337/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5282 - acc: 0.0000e+00 - val_loss: 1.7009 - val_acc: 0.0000e+00\n",
      "Epoch 338/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5137 - acc: 0.0000e+00 - val_loss: 1.6642 - val_acc: 0.0000e+00\n",
      "Epoch 339/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5298 - acc: 0.0000e+00 - val_loss: 1.6891 - val_acc: 0.0000e+00\n",
      "Epoch 340/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5221 - acc: 0.0000e+00 - val_loss: 1.6719 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5080 - acc: 0.0000e+00 - val_loss: 1.6647 - val_acc: 0.0000e+00\n",
      "Epoch 342/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5314 - acc: 0.0000e+00 - val_loss: 1.7099 - val_acc: 0.0000e+00\n",
      "Epoch 343/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5717 - acc: 0.0000e+00 - val_loss: 1.8114 - val_acc: 0.0000e+00\n",
      "Epoch 344/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5273 - acc: 0.0000e+00 - val_loss: 1.7016 - val_acc: 0.0000e+00\n",
      "Epoch 345/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5158 - acc: 0.0000e+00 - val_loss: 1.7487 - val_acc: 0.0000e+00\n",
      "Epoch 346/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5163 - acc: 0.0000e+00 - val_loss: 1.6674 - val_acc: 0.0000e+00\n",
      "Epoch 347/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5067 - acc: 0.0000e+00 - val_loss: 1.7063 - val_acc: 0.0000e+00\n",
      "Epoch 348/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5064 - acc: 0.0000e+00 - val_loss: 1.6610 - val_acc: 0.0000e+00\n",
      "Epoch 349/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5221 - acc: 0.0000e+00 - val_loss: 1.7098 - val_acc: 0.0000e+00\n",
      "Epoch 350/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5375 - acc: 0.0000e+00 - val_loss: 1.6623 - val_acc: 0.0000e+00\n",
      "Epoch 351/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5216 - acc: 0.0000e+00 - val_loss: 1.6972 - val_acc: 0.0000e+00\n",
      "Epoch 352/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5187 - acc: 0.0000e+00 - val_loss: 1.6942 - val_acc: 0.0000e+00\n",
      "Epoch 353/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5214 - acc: 0.0000e+00 - val_loss: 1.8238 - val_acc: 0.0000e+00\n",
      "Epoch 354/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5593 - acc: 0.0000e+00 - val_loss: 1.7016 - val_acc: 0.0000e+00\n",
      "Epoch 355/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5110 - acc: 0.0000e+00 - val_loss: 1.6804 - val_acc: 0.0000e+00\n",
      "Epoch 356/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5304 - acc: 0.0000e+00 - val_loss: 1.6634 - val_acc: 0.0000e+00\n",
      "Epoch 357/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5416 - acc: 0.0000e+00 - val_loss: 1.6768 - val_acc: 0.0000e+00\n",
      "Epoch 358/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5318 - acc: 0.0000e+00 - val_loss: 1.7699 - val_acc: 0.0000e+00\n",
      "Epoch 359/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5166 - acc: 0.0000e+00 - val_loss: 1.7040 - val_acc: 0.0000e+00\n",
      "Epoch 360/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5076 - acc: 0.0000e+00 - val_loss: 1.6847 - val_acc: 0.0000e+00\n",
      "Epoch 361/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5071 - acc: 0.0000e+00 - val_loss: 1.6854 - val_acc: 0.0000e+00\n",
      "Epoch 362/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5394 - acc: 0.0000e+00 - val_loss: 1.6588 - val_acc: 0.0000e+00\n",
      "Epoch 363/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5441 - acc: 0.0000e+00 - val_loss: 1.7928 - val_acc: 0.0000e+00\n",
      "Epoch 364/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5816 - acc: 0.0000e+00 - val_loss: 1.7552 - val_acc: 0.0000e+00\n",
      "Epoch 365/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5221 - acc: 0.0000e+00 - val_loss: 1.6653 - val_acc: 0.0000e+00\n",
      "Epoch 366/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5054 - acc: 0.0000e+00 - val_loss: 1.6993 - val_acc: 0.0000e+00\n",
      "Epoch 367/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5063 - acc: 0.0000e+00 - val_loss: 1.6586 - val_acc: 0.0000e+00\n",
      "Epoch 368/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5206 - acc: 0.0000e+00 - val_loss: 1.6796 - val_acc: 0.0000e+00\n",
      "Epoch 369/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5232 - acc: 0.0000e+00 - val_loss: 1.6651 - val_acc: 0.0000e+00\n",
      "Epoch 370/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5107 - acc: 0.0000e+00 - val_loss: 1.7070 - val_acc: 0.0000e+00\n",
      "Epoch 371/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5109 - acc: 0.0000e+00 - val_loss: 1.6630 - val_acc: 0.0000e+00\n",
      "Epoch 372/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5146 - acc: 0.0000e+00 - val_loss: 1.7072 - val_acc: 0.0000e+00\n",
      "Epoch 373/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5385 - acc: 0.0000e+00 - val_loss: 1.6708 - val_acc: 0.0000e+00\n",
      "Epoch 374/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5131 - acc: 0.0000e+00 - val_loss: 1.6728 - val_acc: 0.0000e+00\n",
      "Epoch 375/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5074 - acc: 0.0000e+00 - val_loss: 1.6600 - val_acc: 0.0000e+00\n",
      "Epoch 376/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5148 - acc: 0.0000e+00 - val_loss: 1.6725 - val_acc: 0.0000e+00\n",
      "Epoch 377/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5070 - acc: 0.0000e+00 - val_loss: 1.6756 - val_acc: 0.0000e+00\n",
      "Epoch 378/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5077 - acc: 0.0000e+00 - val_loss: 1.6581 - val_acc: 0.0000e+00\n",
      "Epoch 379/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5215 - acc: 0.0000e+00 - val_loss: 1.6891 - val_acc: 0.0000e+00\n",
      "Epoch 380/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5087 - acc: 0.0000e+00 - val_loss: 1.6787 - val_acc: 0.0000e+00\n",
      "Epoch 381/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5240 - acc: 0.0000e+00 - val_loss: 1.6614 - val_acc: 0.0000e+00\n",
      "Epoch 382/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5137 - acc: 0.0000e+00 - val_loss: 1.6665 - val_acc: 0.0000e+00\n",
      "Epoch 383/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5069 - acc: 0.0000e+00 - val_loss: 1.6826 - val_acc: 0.0000e+00\n",
      "Epoch 384/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5068 - acc: 0.0000e+00 - val_loss: 1.6786 - val_acc: 0.0000e+00\n",
      "Epoch 385/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5185 - acc: 0.0000e+00 - val_loss: 1.6566 - val_acc: 0.0000e+00\n",
      "Epoch 386/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5339 - acc: 0.0000e+00 - val_loss: 1.6833 - val_acc: 0.0000e+00\n",
      "Epoch 387/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5142 - acc: 0.0000e+00 - val_loss: 1.6698 - val_acc: 0.0000e+00\n",
      "Epoch 388/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5166 - acc: 0.0000e+00 - val_loss: 1.7571 - val_acc: 0.0000e+00\n",
      "Epoch 389/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5141 - acc: 0.0000e+00 - val_loss: 1.7154 - val_acc: 0.0000e+00\n",
      "Epoch 390/500\n",
      "698/698 [==============================] - 0s 41us/step - loss: 1.5266 - acc: 0.0000e+00 - val_loss: 1.7091 - val_acc: 0.0000e+00\n",
      "Epoch 391/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5398 - acc: 0.0000e+00 - val_loss: 1.6622 - val_acc: 0.0000e+00\n",
      "Epoch 392/500\n",
      "698/698 [==============================] - 0s 41us/step - loss: 1.5177 - acc: 0.0000e+00 - val_loss: 1.7278 - val_acc: 0.0000e+00\n",
      "Epoch 393/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5140 - acc: 0.0000e+00 - val_loss: 1.7645 - val_acc: 0.0000e+00\n",
      "Epoch 394/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5048 - acc: 0.0000e+00 - val_loss: 1.7397 - val_acc: 0.0000e+00\n",
      "Epoch 395/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5094 - acc: 0.0000e+00 - val_loss: 1.6883 - val_acc: 0.0000e+00\n",
      "Epoch 396/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5210 - acc: 0.0000e+00 - val_loss: 1.6550 - val_acc: 0.0000e+00\n",
      "Epoch 397/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5104 - acc: 0.0000e+00 - val_loss: 1.6564 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5135 - acc: 0.0000e+00 - val_loss: 1.6913 - val_acc: 0.0000e+00\n",
      "Epoch 399/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5378 - acc: 0.0000e+00 - val_loss: 1.6702 - val_acc: 0.0000e+00\n",
      "Epoch 400/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5135 - acc: 0.0000e+00 - val_loss: 1.6790 - val_acc: 0.0000e+00\n",
      "Epoch 401/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5100 - acc: 0.0000e+00 - val_loss: 1.6617 - val_acc: 0.0000e+00\n",
      "Epoch 402/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5001 - acc: 0.0000e+00 - val_loss: 1.6882 - val_acc: 0.0000e+00\n",
      "Epoch 403/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5276 - acc: 0.0000e+00 - val_loss: 1.6618 - val_acc: 0.0000e+00\n",
      "Epoch 404/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5001 - acc: 0.0000e+00 - val_loss: 1.7725 - val_acc: 0.0000e+00\n",
      "Epoch 405/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5229 - acc: 0.0000e+00 - val_loss: 1.7122 - val_acc: 0.0000e+00\n",
      "Epoch 406/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5117 - acc: 0.0000e+00 - val_loss: 1.6533 - val_acc: 0.0000e+00\n",
      "Epoch 407/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5078 - acc: 0.0000e+00 - val_loss: 1.6642 - val_acc: 0.0000e+00\n",
      "Epoch 408/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5191 - acc: 0.0000e+00 - val_loss: 1.6606 - val_acc: 0.0000e+00\n",
      "Epoch 409/500\n",
      "698/698 [==============================] - 0s 41us/step - loss: 1.5115 - acc: 0.0000e+00 - val_loss: 1.6837 - val_acc: 0.0000e+00\n",
      "Epoch 410/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5246 - acc: 0.0000e+00 - val_loss: 1.7382 - val_acc: 0.0000e+00\n",
      "Epoch 411/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5106 - acc: 0.0000e+00 - val_loss: 1.7329 - val_acc: 0.0000e+00\n",
      "Epoch 412/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5033 - acc: 0.0000e+00 - val_loss: 1.7775 - val_acc: 0.0000e+00\n",
      "Epoch 413/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5185 - acc: 0.0000e+00 - val_loss: 1.6683 - val_acc: 0.0000e+00\n",
      "Epoch 414/500\n",
      "698/698 [==============================] - ETA: 0s - loss: 1.4622 - acc: 0.0000e+0 - 0s 34us/step - loss: 1.5015 - acc: 0.0000e+00 - val_loss: 1.6935 - val_acc: 0.0000e+00\n",
      "Epoch 415/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5023 - acc: 0.0000e+00 - val_loss: 1.6806 - val_acc: 0.0000e+00\n",
      "Epoch 416/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.4997 - acc: 0.0000e+00 - val_loss: 1.7031 - val_acc: 0.0000e+00\n",
      "Epoch 417/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5039 - acc: 0.0000e+00 - val_loss: 1.6542 - val_acc: 0.0000e+00\n",
      "Epoch 418/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5579 - acc: 0.0000e+00 - val_loss: 1.6660 - val_acc: 0.0000e+00\n",
      "Epoch 419/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5183 - acc: 0.0000e+00 - val_loss: 1.6641 - val_acc: 0.0000e+00\n",
      "Epoch 420/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5011 - acc: 0.0000e+00 - val_loss: 1.6498 - val_acc: 0.0000e+00\n",
      "Epoch 421/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5063 - acc: 0.0000e+00 - val_loss: 1.6511 - val_acc: 0.0000e+00\n",
      "Epoch 422/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5168 - acc: 0.0000e+00 - val_loss: 1.6734 - val_acc: 0.0000e+00\n",
      "Epoch 423/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5295 - acc: 0.0000e+00 - val_loss: 1.6991 - val_acc: 0.0000e+00\n",
      "Epoch 424/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5294 - acc: 0.0000e+00 - val_loss: 1.7314 - val_acc: 0.0000e+00\n",
      "Epoch 425/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5028 - acc: 0.0000e+00 - val_loss: 1.6572 - val_acc: 0.0000e+00\n",
      "Epoch 426/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5395 - acc: 0.0000e+00 - val_loss: 1.6508 - val_acc: 0.0000e+00\n",
      "Epoch 427/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5077 - acc: 0.0000e+00 - val_loss: 1.6503 - val_acc: 0.0000e+00\n",
      "Epoch 428/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5039 - acc: 0.0000e+00 - val_loss: 1.7644 - val_acc: 0.0000e+00\n",
      "Epoch 429/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5149 - acc: 0.0000e+00 - val_loss: 1.6982 - val_acc: 0.0000e+00\n",
      "Epoch 430/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.5130 - acc: 0.0000e+00 - val_loss: 1.6478 - val_acc: 0.0000e+00\n",
      "Epoch 431/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5060 - acc: 0.0000e+00 - val_loss: 1.6634 - val_acc: 0.0000e+00\n",
      "Epoch 432/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5018 - acc: 0.0000e+00 - val_loss: 1.6732 - val_acc: 0.0000e+00\n",
      "Epoch 433/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5008 - acc: 0.0000e+00 - val_loss: 1.6757 - val_acc: 0.0000e+00\n",
      "Epoch 434/500\n",
      "698/698 [==============================] - 0s 41us/step - loss: 1.4968 - acc: 0.0000e+00 - val_loss: 1.6680 - val_acc: 0.0000e+00\n",
      "Epoch 435/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5013 - acc: 0.0000e+00 - val_loss: 1.6599 - val_acc: 0.0000e+00\n",
      "Epoch 436/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5252 - acc: 0.0000e+00 - val_loss: 1.6604 - val_acc: 0.0000e+00\n",
      "Epoch 437/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5055 - acc: 0.0000e+00 - val_loss: 1.6885 - val_acc: 0.0000e+00\n",
      "Epoch 438/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5319 - acc: 0.0000e+00 - val_loss: 1.6476 - val_acc: 0.0000e+00\n",
      "Epoch 439/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5637 - acc: 0.0000e+00 - val_loss: 1.8059 - val_acc: 0.0000e+00\n",
      "Epoch 440/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5292 - acc: 0.0000e+00 - val_loss: 1.7380 - val_acc: 0.0000e+00\n",
      "Epoch 441/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5132 - acc: 0.0000e+00 - val_loss: 1.6753 - val_acc: 0.0000e+00\n",
      "Epoch 442/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.4969 - acc: 0.0000e+00 - val_loss: 1.6484 - val_acc: 0.0000e+00\n",
      "Epoch 443/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5195 - acc: 0.0000e+00 - val_loss: 1.6562 - val_acc: 0.0000e+00\n",
      "Epoch 444/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5371 - acc: 0.0000e+00 - val_loss: 1.7926 - val_acc: 0.0000e+00\n",
      "Epoch 445/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.5243 - acc: 0.0000e+00 - val_loss: 1.6557 - val_acc: 0.0000e+00\n",
      "Epoch 446/500\n",
      "698/698 [==============================] - 0s 30us/step - loss: 1.4995 - acc: 0.0000e+00 - val_loss: 1.6473 - val_acc: 0.0000e+00\n",
      "Epoch 447/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5321 - acc: 0.0000e+00 - val_loss: 1.6471 - val_acc: 0.0000e+00\n",
      "Epoch 448/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5015 - acc: 0.0000e+00 - val_loss: 1.7045 - val_acc: 0.0000e+00\n",
      "Epoch 449/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5180 - acc: 0.0000e+00 - val_loss: 1.6709 - val_acc: 0.0000e+00\n",
      "Epoch 450/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5212 - acc: 0.0000e+00 - val_loss: 1.6932 - val_acc: 0.0000e+00\n",
      "Epoch 451/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5201 - acc: 0.0000e+00 - val_loss: 1.7469 - val_acc: 0.0000e+00\n",
      "Epoch 452/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5065 - acc: 0.0000e+00 - val_loss: 1.7225 - val_acc: 0.0000e+00\n",
      "Epoch 453/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5003 - acc: 0.0000e+00 - val_loss: 1.6900 - val_acc: 0.0000e+00\n",
      "Epoch 454/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698/698 [==============================] - 0s 32us/step - loss: 1.5037 - acc: 0.0000e+00 - val_loss: 1.6596 - val_acc: 0.0000e+00\n",
      "Epoch 455/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5066 - acc: 0.0000e+00 - val_loss: 1.7068 - val_acc: 0.0000e+00\n",
      "Epoch 456/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5023 - acc: 0.0000e+00 - val_loss: 1.6781 - val_acc: 0.0000e+00\n",
      "Epoch 457/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.4962 - acc: 0.0000e+00 - val_loss: 1.6697 - val_acc: 0.0000e+00\n",
      "Epoch 458/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.4929 - acc: 0.0000e+00 - val_loss: 1.6591 - val_acc: 0.0000e+00\n",
      "Epoch 459/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5097 - acc: 0.0000e+00 - val_loss: 1.6573 - val_acc: 0.0000e+00\n",
      "Epoch 460/500\n",
      "698/698 [==============================] - 0s 39us/step - loss: 1.5107 - acc: 0.0000e+00 - val_loss: 1.7263 - val_acc: 0.0000e+00\n",
      "Epoch 461/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5083 - acc: 0.0000e+00 - val_loss: 1.6513 - val_acc: 0.0000e+00\n",
      "Epoch 462/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.4890 - acc: 0.0000e+00 - val_loss: 1.7010 - val_acc: 0.0000e+00\n",
      "Epoch 463/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.4966 - acc: 0.0000e+00 - val_loss: 1.6604 - val_acc: 0.0000e+00\n",
      "Epoch 464/500\n",
      "698/698 [==============================] - 0s 29us/step - loss: 1.4980 - acc: 0.0000e+00 - val_loss: 1.6776 - val_acc: 0.0000e+00\n",
      "Epoch 465/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.4988 - acc: 0.0000e+00 - val_loss: 1.6804 - val_acc: 0.0000e+00\n",
      "Epoch 466/500\n",
      "698/698 [==============================] - 0s 40us/step - loss: 1.4972 - acc: 0.0000e+00 - val_loss: 1.6727 - val_acc: 0.0000e+00\n",
      "Epoch 467/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.4984 - acc: 0.0000e+00 - val_loss: 1.6539 - val_acc: 0.0000e+00\n",
      "Epoch 468/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5038 - acc: 0.0000e+00 - val_loss: 1.7357 - val_acc: 0.0000e+00\n",
      "Epoch 469/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.4976 - acc: 0.0000e+00 - val_loss: 1.6481 - val_acc: 0.0000e+00\n",
      "Epoch 470/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5095 - acc: 0.0000e+00 - val_loss: 1.6986 - val_acc: 0.0000e+00\n",
      "Epoch 471/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5192 - acc: 0.0000e+00 - val_loss: 1.6976 - val_acc: 0.0000e+00\n",
      "Epoch 472/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5091 - acc: 0.0000e+00 - val_loss: 1.6879 - val_acc: 0.0000e+00\n",
      "Epoch 473/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5290 - acc: 0.0000e+00 - val_loss: 1.6467 - val_acc: 0.0000e+00\n",
      "Epoch 474/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5261 - acc: 0.0000e+00 - val_loss: 1.6625 - val_acc: 0.0000e+00\n",
      "Epoch 475/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5124 - acc: 0.0000e+00 - val_loss: 1.7237 - val_acc: 0.0000e+00\n",
      "Epoch 476/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5076 - acc: 0.0000e+00 - val_loss: 1.7027 - val_acc: 0.0000e+00\n",
      "Epoch 477/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.5088 - acc: 0.0000e+00 - val_loss: 1.7805 - val_acc: 0.0000e+00\n",
      "Epoch 478/500\n",
      "698/698 [==============================] - 0s 31us/step - loss: 1.5359 - acc: 0.0000e+00 - val_loss: 1.7036 - val_acc: 0.0000e+00\n",
      "Epoch 479/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5505 - acc: 0.0000e+00 - val_loss: 1.6634 - val_acc: 0.0000e+00\n",
      "Epoch 480/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5014 - acc: 0.0000e+00 - val_loss: 1.6973 - val_acc: 0.0000e+00\n",
      "Epoch 481/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5040 - acc: 0.0000e+00 - val_loss: 1.7100 - val_acc: 0.0000e+00\n",
      "Epoch 482/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.5010 - acc: 0.0000e+00 - val_loss: 1.6534 - val_acc: 0.0000e+00\n",
      "Epoch 483/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.4926 - acc: 0.0000e+00 - val_loss: 1.6603 - val_acc: 0.0000e+00\n",
      "Epoch 484/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5066 - acc: 0.0000e+00 - val_loss: 1.6447 - val_acc: 0.0000e+00\n",
      "Epoch 485/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5170 - acc: 0.0000e+00 - val_loss: 1.7025 - val_acc: 0.0000e+00\n",
      "Epoch 486/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.4933 - acc: 0.0000e+00 - val_loss: 1.6681 - val_acc: 0.0000e+00\n",
      "Epoch 487/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.4983 - acc: 0.0000e+00 - val_loss: 1.6802 - val_acc: 0.0000e+00\n",
      "Epoch 488/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.4957 - acc: 0.0000e+00 - val_loss: 1.6763 - val_acc: 0.0000e+00\n",
      "Epoch 489/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5010 - acc: 0.0000e+00 - val_loss: 1.6865 - val_acc: 0.0000e+00\n",
      "Epoch 490/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.4942 - acc: 0.0000e+00 - val_loss: 1.6604 - val_acc: 0.0000e+00\n",
      "Epoch 491/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.4950 - acc: 0.0000e+00 - val_loss: 1.6713 - val_acc: 0.0000e+00\n",
      "Epoch 492/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5005 - acc: 0.0000e+00 - val_loss: 1.6753 - val_acc: 0.0000e+00\n",
      "Epoch 493/500\n",
      "698/698 [==============================] - 0s 38us/step - loss: 1.5104 - acc: 0.0000e+00 - val_loss: 1.6434 - val_acc: 0.0000e+00\n",
      "Epoch 494/500\n",
      "698/698 [==============================] - 0s 34us/step - loss: 1.5292 - acc: 0.0000e+00 - val_loss: 1.6811 - val_acc: 0.0000e+00\n",
      "Epoch 495/500\n",
      "698/698 [==============================] - 0s 37us/step - loss: 1.5152 - acc: 0.0000e+00 - val_loss: 1.7030 - val_acc: 0.0000e+00\n",
      "Epoch 496/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5034 - acc: 0.0000e+00 - val_loss: 1.6861 - val_acc: 0.0000e+00\n",
      "Epoch 497/500\n",
      "698/698 [==============================] - 0s 36us/step - loss: 1.5096 - acc: 0.0000e+00 - val_loss: 1.6456 - val_acc: 0.0000e+00\n",
      "Epoch 498/500\n",
      "698/698 [==============================] - 0s 32us/step - loss: 1.4948 - acc: 0.0000e+00 - val_loss: 1.6623 - val_acc: 0.0000e+00\n",
      "Epoch 499/500\n",
      "698/698 [==============================] - 0s 33us/step - loss: 1.4959 - acc: 0.0000e+00 - val_loss: 1.6426 - val_acc: 0.0000e+00\n",
      "Epoch 500/500\n",
      "698/698 [==============================] - 0s 35us/step - loss: 1.5048 - acc: 0.0000e+00 - val_loss: 1.6720 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#opt =  Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer = \"adam\", loss = \"mean_absolute_error\", metrics = [\"accuracy\"])\n",
    "history = model.fit(x = X_train.values, y = y_train.values, epochs = 500, \n",
    "                    verbose=1, batch_size = 32, shuffle=True, \n",
    "                    validation_data = (X_test.values, y_test.values) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s 22us/step\n",
      "\n",
      "Loss = 1.6719764021464756\n",
      "Test Accuracy = 0.0\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (1 line)\n",
    "preds = model.evaluate(X_test.values, y_test.values)\n",
    "### END CODE HERE ###\n",
    "print()\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.01523345,  0.68654424,  0.05506377],\n",
       "        [ 0.3932423 ,  0.62106925,  0.9738478 ],\n",
       "        [ 0.7166254 ,  1.3963885 , -0.11444313]], dtype=float32),\n",
       " array([1.5747631, 1.5654247, 1.6217307], dtype=float32),\n",
       " array([[0.6290362],\n",
       "        [0.3613625],\n",
       "        [1.0678002]], dtype=float32),\n",
       " array([1.5132896], dtype=float32)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "Keras可以大大加速构建神经网络，新手从Keras开始可以有效的降低Tensorflow学习曲线的陡度。使用Keras的model基于一层层的层叠加，可扩展性非常，可以看到Tensorflow要用很多行代码用keras几行就可以完成，而且层次明了。当然Keras隐藏掉许多实现细节，可能会导致对于模型的理解不够深入，另外一些复杂的模型还需要Tensorflow底层功能，使用Keras的同时建议也要对Tensorflow有一定了解。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
