{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding 学习笔记1\n",
    "\n",
    "自然语言处理(Nature Language Processing:NLP)一直是机器学习一个重要研究方向，而计算机要理解自然语言，首先要把语言转化成计算机能理解的形式。\n",
    "\n",
    "编码单词的一种方法是One Hot(OH)，首先收集样本中的所有单词生成字典，然后给它们编号。比如有1000个单词，那每个单词就有从$[0, 999]$的一个数字给它编号，具体的编号并不重要，主要是编号要能与词典中的单词对应。这样每个单词就可以由一个$(1000,1)$的向量来表示，第$k$个单词转化为向量在$(k,1)$处为1，其它地方全为0。\n",
    "\n",
    "这种表示方法也大量应用在机器学习中，可以生成词袋模型(Bag of Word)，一般还要进行TF-IDF(Term Frequency - Inverse Document Frequency)处理，然后就可以输入模型。\n",
    "\n",
    "我们知道自然语言中大量的单词都是相关的，然而OH却只能提取的词频信息，一种补救的办法就是N-Gram——使用词组。1-Gram只使用一个单词，2-Gram时要使用一个2单词的窗口在样本中提取2词词组放入字典，以此类推，N-Gram就是使用N个单词的窗口。多词词组一般不单独使用，都是从1到N Gram，比如N=2时，字典中就包含样本中的单词和双词词组。\n",
    "\n",
    "N-Gram会导致计算量急剧上升，而且不能捕捉到间隔单词间的联系，导致应用受限。近些年来，在深度学习领域使用一种新的单词表示方法——词嵌入(Word Embedding)模型，实践表明词嵌入模型具有很多优势，能够显著提升自然语言处理的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding 模型\n",
    "\n",
    "词嵌入指把单词映射到一个高维的空间$E:words -> R^n$，维度可以是50，100或者300。比如$$ E(\"cat\")=(0.2, -0.4, 0.7, ...)$$ $$E(\"mat\") = (0.0, 0.6, -0.1, ...)$$\n",
    "\n",
    "$E$是一个系数矩阵，每一列是一个词的向量，如果字典中有1000个单词，把单词嵌入50维的向量空间中，那$E$就是$(50,1000)$维的矩阵。可以使用自己的样本学习得出$E$，但通常事先在大样本中训练好的$E$，然后直接使用。\n",
    "\n",
    "### 提取$E$矩阵\n",
    "\n",
    "通常有两种方法提取出词向量矩阵$E$：Continuous Bag Of Words(CBOW)和Skip-Gram。两种方法都先设定一个窗口(Windows Size)大小，通常是5。CBOW在窗口中选取一个目标单词，然后用其它词去预测目标单词出现的概率；而Skip-Gram正好相反，是用选取一个目标单词去预测其它词出现的概率(参考[blog](https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b))\n",
    "\n",
    "<img src=\"images/skipgram_window.png\" style=\"width:400px;\">\n",
    "<caption><center> **Figure 1**: Skip-Gram window </center></caption>\n",
    "\n",
    "上图显示的是Skip-Gram使用一个$windows size = 5 $窗口在训练样本中滑动生成的训练样本情况。这样样本然后输入一个简单的网络中训练后就可以得出矩阵$E$。模型中只有一个隐藏层，该层没有激活函数，输出的激活函数为$softmax$\n",
    "\n",
    "<img src=\"images/skipgram_model.png\" style=\"width:400px;\">\n",
    "<caption><center> **Figure 1**: Skip-Gram Model </center></caption>\n",
    "\n",
    "如果两个向量接近，则意味着它们在训练集中同时出现的概率很高。也正是因为这种特性，Word Embedding有很多有意思的应用，比如我们可以通过向量的距离找出相似的词，如果给出man->woman, king->?，模型则会找到queen来对应。\n",
    "\n",
    "词向量既能够降低维度，又能够捕捉到当前词在本句子中上下文的信息（表现为前后距离关系），那么对其用来表示语言句子词语作为神经网络的输入效果通常比较让人满意。\n",
    "\n",
    "本系列后续还有几篇文章将陆续深入介绍词向量。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
