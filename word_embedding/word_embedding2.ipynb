{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding 2 - CBOW\n",
    "\n",
    "CBOW(Continuous Bag of Words)是一种广泛使用的词嵌入模型，最初是由Mikolov提出。CBOW中由周围词(surrounding words)去预测目标词(target words)。以这个句子为例** the quick brown fox jumps over the lazy dog **，如果窗口长度为2，提取出的样本格式**([context words], target)**，则可以提取出**([the brown], quick)**, **([quick, fox], brown)**,..., **([the dog], lazy)**等训练样本。\n",
    "\n",
    "<img src=\"images/cbow_sample.png\" style=\"width:500px;\">\n",
    "<caption><center> **Figure 1**: CBOW sample </center></caption>\n",
    "\n",
    "\n",
    "这些训练样本就可以输入只有一个隐藏层简单的DNN(Deep Neural Network)中进行训练得出word embedding向量\n",
    "\n",
    "<img src=\"images/cbow_model.png\" style=\"width:300px;\">\n",
    "<caption><center> **Figure 1**: CBOW model </center></caption>\n",
    "\n",
    "输入为context words，经过embedding层后，得到每个单词的embedding vetor，然后加和，再通过输出softmax计算预测目标词的概率$\\hat{y}$，损失函数为$-\\sum{y*log(\\hat{y})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW 模型实现\n",
    "\n",
    "下面我们来看一个具体实现的例子，内容主要来自于这篇[blog](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)，感谢Dipanjan Sarkar。一些现成的框架已经有模型的实现，比如gensim，但本文为了介绍实现原理，我们来build from scratch。本例中使用了Bible corpus，实现过程有以下几部分：\n",
    "\n",
    "* 预处理(Pre-processing)\n",
    "* 提取文本字典(Build the corpus vocabulary)\n",
    "* CBOW训练集生成器(Build a CBOW generator)\n",
    "* 基于Keras构建CBOW模型(Build the CBOW model with keras)\n",
    "* 训练模型(Train the model)\n",
    "* 获取词向量(Get Word Embeddings)\n",
    "\n",
    "\n",
    "### 预处理\n",
    "导入库，并实现一个预处理器，主要是将文本转为小写，剔除掉文本中的特殊字符等处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载训练文本\n",
    "\n",
    "训练文本中一共有30103行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 30103\n",
      "\n",
      "Sample line: ['1', ':', '6', 'And', 'God', 'said', ',', 'Let', 'there', 'be', 'a', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let', 'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']\n",
      "\n",
      "Processed line: god said let firmament midst waters let divide waters waters\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation\n",
    "\n",
    "bible = gutenberg.sents('bible-kjv.txt') \n",
    "remove_terms = punctuation + '0123456789'\n",
    "\n",
    "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
    "\n",
    "print('Total lines:', len(bible))\n",
    "print('\\nSample line:', bible[10])\n",
    "print('\\nProcessed line:', norm_bible[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取文本字典\n",
    "\n",
    "从训练文本集中corpus提取所有单词，给每一个一个编号做为字典。最后生成的字典长度为12425，每个单词都有一个唯一的编号，比如shall编号为1，lord为3。字典中还加入一个'PAD'词，编号为0，用于补全句前和句尾截断的样本集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12425\n",
      "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW训练集生成器\n",
    "\n",
    "我们使用一个长度为** 2 x windows **的窗口在每个句子中滑动提取样本集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word   = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i] \n",
    "                                 for i in range(start, end) \n",
    "                                 if 0 <= i < sentence_length \n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "来看一下训练集的情况(剔除了句前和句后的训练器）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
      "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
      "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
      "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
      "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
      "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
      "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
      "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
      "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
      "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
      "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如**old testament king james bible**一个样本就是 **( ['old', 'testament', 'james', 'bible'], king) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Keras建立CBOW模型\n",
    "我们使用基于TensorFlow的Keras建立模型。输入层的输入为长度为4的样本Context，随机初始化Word Embedding，输出为context中的embedding向量。然后经过一个平均值计算平均值，再输入softmax激活函数中，输出就是一个预测Target词出现概率的向量，该向量与实际输出结合使用 **categorical_crossentropy**计算开销输入到模型中进行优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            1242500   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12425)             1254925   \n",
      "=================================================================\n",
      "Total params: 2,497,425\n",
      "Trainable params: 2,497,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"288pt\" viewBox=\"0.00 0.00 241.86 288.00\" width=\"242pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 284)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-284 237.8623,-284 237.8623,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4872474016 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4872474016</title>\n",
       "<polygon fill=\"none\" points=\"0,-162.5 0,-206.5 233.8623,-206.5 233.8623,-162.5 0,-162.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40.2725\" y=\"-180.3\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"80.5449,-162.5 80.5449,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108.3794\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"80.5449,-184.5 136.2139,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"108.3794\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"136.2139,-162.5 136.2139,-206.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185.0381\" y=\"-191.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"136.2139,-184.5 233.8623,-184.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"185.0381\" y=\"-169.3\">(None, 4, 100)</text>\n",
       "</g>\n",
       "<!-- 4872472952 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4872472952</title>\n",
       "<polygon fill=\"none\" points=\"9.3379,-81.5 9.3379,-125.5 224.5244,-125.5 224.5244,-81.5 9.3379,-81.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40.2725\" y=\"-99.3\">Lambda</text>\n",
       "<polyline fill=\"none\" points=\"71.207,-81.5 71.207,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99.0415\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"71.207,-103.5 126.876,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"99.0415\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"126.876,-81.5 126.876,-125.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.7002\" y=\"-110.3\">(None, 4, 100)</text>\n",
       "<polyline fill=\"none\" points=\"126.876,-103.5 224.5244,-103.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.7002\" y=\"-88.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 4872474016&#45;&gt;4872472952 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4872474016-&gt;4872472952</title>\n",
       "<path d=\"M116.9312,-162.3664C116.9312,-154.1516 116.9312,-144.6579 116.9312,-135.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"120.4313,-135.6068 116.9312,-125.6068 113.4313,-135.6069 120.4313,-135.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4872473680 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4872473680</title>\n",
       "<polygon fill=\"none\" points=\"14.7793,-.5 14.7793,-44.5 219.083,-44.5 219.083,-.5 14.7793,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"40.2725\" y=\"-18.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"65.7656,-.5 65.7656,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93.6001\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"65.7656,-22.5 121.4346,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93.6001\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"121.4346,-.5 121.4346,-44.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.2588\" y=\"-29.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"121.4346,-22.5 219.083,-22.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.2588\" y=\"-7.3\">(None, 12425)</text>\n",
       "</g>\n",
       "<!-- 4872472952&#45;&gt;4872473680 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4872472952-&gt;4872473680</title>\n",
       "<path d=\"M116.9312,-81.3664C116.9312,-73.1516 116.9312,-63.6579 116.9312,-54.7252\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"120.4313,-54.6068 116.9312,-44.6068 113.4313,-54.6069 120.4313,-54.6068\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4872473736 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4872473736</title>\n",
       "<polygon fill=\"none\" points=\"73.9312,-243.5 73.9312,-279.5 159.9312,-279.5 159.9312,-243.5 73.9312,-243.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"116.9312\" y=\"-257.3\">4872473736</text>\n",
       "</g>\n",
       "<!-- 4872473736&#45;&gt;4872474016 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4872473736-&gt;4872474016</title>\n",
       "<path d=\"M116.9312,-243.2521C116.9312,-235.3888 116.9312,-225.9498 116.9312,-216.9612\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"120.4313,-216.7376 116.9312,-206.7377 113.4313,-216.7377 120.4313,-216.7376\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
    "                 rankdir='TB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "提取word embedding非常耗时，使用AWS P2.x instance with GPU也得需要1.5个小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取词向量\n",
    "最后我们就可以获得词向量，每个单词对应一个100维的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cbow_embedding.png\" style=\"width:700px;\">\n",
    "<caption><center> **Figure 1**: Word Embedding </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
